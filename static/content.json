{
    "abstract": {
        "full_text": "Feedback on user interface (UI) mockups is crucial in design. However, human feedback is not always readily available. We explore the potential of using large language models for automatic feedback. Specifically, we focus on applying GPT-4 to automate heuristic evaluation, which currently entails a human expert assessing a UI's compliance with a set of design guidelines. We implemented a Figma plugin that takes in a UI design and a set of written heuristics, and renders automatically-generated feedback as constructive suggestions. We assessed performance on 51 UIs using three sets of guidelines, compared GPT-4-generated design suggestions with those from human experts, and conducted a study with 12 expert designers to understand fit with existing practice. We found that GPT-4-based feedback is useful for catching subtle errors, improving text, and considering UI semantics, but feedback also decreased in utility over iterations. Participants described several uses for this plugin despite its imperfect suggestions.",
        "sentences": [
            {
                "id": "abstract_s1",
                "text": "Feedback on user interface (UI) mockups is crucial in design.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s2",
                "text": "However, human feedback is not always readily available.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s3",
                "text": "We explore the potential of using large language models for automatic feedback.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s4",
                "text": "Specifically, we focus on applying GPT-4 to automate heuristic evaluation, which currently entails a human expert assessing a UI's compliance with a set of design guidelines.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s5",
                "text": "We implemented a Figma plugin that takes in a UI design and a set of written heuristics, and renders automatically-generated feedback as constructive suggestions.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s6",
                "text": "We assessed performance on 51 UIs using three sets of guidelines, compared GPT-4-generated design suggestions with those from human experts, and conducted a study with 12 expert designers to understand fit with existing practice.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s7",
                "text": "We found that GPT-4-based feedback is useful for catching subtle errors, improving text, and considering UI semantics, but feedback also decreased in utility over iterations.",
                "context": "Abstract",
                "associated_visual": "fig1"
            },
            {
                "id": "abstract_s8",
                "text": "Participants described several uses for this plugin despite its imperfect suggestions.",
                "context": "Abstract",
                "associated_visual": "fig1"
            }
        ]
    },
    "body": {
        "sections": [
            {
                "id": "sec-2",
                "section_number": "1",
                "title": "INTRODUCTION",
                "paragraphs": [
                    {
                        "full_text": "User interface (UI) design is an essential domain that shapes how humans interact with technology and digital information. Designing user interfaces commonly involves iterative rounds of feedback and revision. Feedback is essential for guiding designers towards improving their UIs. While this feedback traditionally comes from humans (via user studies and expert evaluations), recent advances in computational UI design enable automated feedback. However, automated feedback is often limited in scope (e.g., the metric could only evaluate layout complexity) and can be challenging to interpret [50]. While human feedback is more informative, it is not readily available and requires time and resources for recruiting and compensating participants.",
                        "sentences": [
                            {
                                "id": "sec-2_p1_s1",
                                "text": "User interface (UI) design is an essential domain that shapes how humans interact with technology and digital information.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p1_s2",
                                "text": "Designing user interfaces commonly involves iterative rounds of feedback and revision.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p1_s3",
                                "text": "Feedback is essential for guiding designers towards improving their UIs.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p1_s4",
                                "text": "While this feedback traditionally comes from humans (via user studies and expert evaluations), recent advances in computational UI design enable automated feedback.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p1_s5",
                                "text": "However, automated feedback is often limited in scope (e.g., the metric could only evaluate layout complexity) and can be challenging to interpret [50].",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p1_s6",
                                "text": "While human feedback is more informative, it is not readily available and requires time and resources for recruiting and compensating participants.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "One method of evaluation that still relies on human participants today is heuristic evaluation, where an experienced evaluator checks an interface against a list of usability heuristics (rules of thumb) developed over time, such as Nielsen's 10 Usability Heuristics [39]. Despite appearing straightforward, heuristic evaluation is challenging and subjective [40], dependent on the evaluator's previous training and personality-related factors [25]. These limitations further suggest an opportunity for AI-assisted evaluation.",
                        "sentences": [
                            {
                                "id": "sec-2_p2_s1",
                                "text": "One method of evaluation that still relies on human participants today is heuristic evaluation, where an experienced evaluator checks an interface against a list of usability heuristics (rules of thumb) developed over time, such as Nielsen's 10 Usability Heuristics [39].",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p2_s2",
                                "text": "Despite appearing straightforward, heuristic evaluation is challenging and subjective [40], dependent on the evaluator's previous training and personality-related factors [25].",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p2_s3",
                                "text": "These limitations further suggest an opportunity for AI-assisted evaluation.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "There are several reasons why LLMs could be suitable for automating heuristic evaluation. The evaluation process primarily involves rule-based reasoning, which LLMs have shown capacity for [42]. Moreover, design guidelines are predominately in text form, making them amenable for LLMs, and the language model could also return its feedback as text-based explanations that designers prefer [23]. Finally, LLMs have demonstrated the ability to understand and reason with mobile UIs [56], as well as generalize to new tasks and data [28, 49]. However, there are also reasons that suggest caution for using LLMs for this task. For one, LLMs only accept text as input, while user interfaces are complex artifacts that combine text, images, and UI components into hierarchical layouts. In addition, LLMs have been shown to hallucinate [24] (i.e., generate false information) and may potentially identify incorrect guideline violations. This paper explores the potential of using LLMs to carry out heuristic evaluation automatically. In particular, we aim to determine their performance, strengths and limitations, and how an LLM-based tool can fit into existing design practices.",
                        "sentences": [
                            {
                                "id": "sec-2_p3_s1",
                                "text": "There are several reasons why LLMs could be suitable for automating heuristic evaluation.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s2",
                                "text": "The evaluation process primarily involves rule-based reasoning, which LLMs have shown capacity for [42].",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s3",
                                "text": "Moreover, design guidelines are predominately in text form, making them amenable for LLMs, and the language model could also return its feedback as text-based explanations that designers prefer [23].",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s4",
                                "text": "Finally, LLMs have demonstrated the ability to understand and reason with mobile UIs [56], as well as generalize to new tasks and data [28, 49].",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s5",
                                "text": "However, there are also reasons that suggest caution for using LLMs for this task.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s6",
                                "text": "For one, LLMs only accept text as input, while user interfaces are complex artifacts that combine text, images, and UI components into hierarchical layouts.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s7",
                                "text": "In addition, LLMs have been shown to hallucinate [24] (i.e., generate false information) and may potentially identify incorrect guideline violations.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s8",
                                "text": "This paper explores the potential of using LLMs to carry out heuristic evaluation automatically.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p3_s9",
                                "text": "In particular, we aim to determine their performance, strengths and limitations, and how an LLM-based tool can fit into existing design practices.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "To explore the potential of LLMs in conducting heuristic evaluation, we built a tool that enables designers to run automatic evaluations on UI mockups and receive text-based feedback. We package this system as a plugin for Figma [1], a popular UI design tool. Figure 1 illustrates the iterative usage of this plugin. The designer prototypes their UI in Figma, and then selects a set of guidelines they would like to use for evaluation in our plugin. The plugin returns the feedback, which the designer uses to revise their mockup. The designer can then repeat this process on their edited mockup. To improve the LLM's performance and adapt to individual preferences, designers can provide feedback on each generated suggestion, which is integrated into the model for the next round of evaluation. The plugin produces UI mockup feedback by querying an LLM with the guidelines\u2019 text and a JSON representation of the UI. The LLM then returns a set of detected guideline violations. Instead of directly stating the violations, they are phrased as constructive suggestions for improving the UI. As LLMs can only process text and have a limited context window, we developed a JSON representation of the UI that concisely captures the layout hierarchy and contains both semantic (text, semantic label, element type) and visual (location, size, and color) details of each element and group in the UI. To further accommodate context window limits, we scoped the plugin to evaluate only static (i.e., non-interactive) UI mockups, one screen at a time.",
                        "sentences": [
                            {
                                "id": "sec-2_p4_s1",
                                "text": "To explore the potential of LLMs in conducting heuristic evaluation, we built a tool that enables designers to run automatic evaluations on UI mockups and receive text-based feedback.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s2",
                                "text": "We package this system as a plugin for Figma [1], a popular UI design tool.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s3",
                                "text": "Figure 1 illustrates the iterative usage of this plugin.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s4",
                                "text": "The designer prototypes their UI in Figma, and then selects a set of guidelines they would like to use for evaluation in our plugin.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s5",
                                "text": "The plugin returns the feedback, which the designer uses to revise their mockup.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s6",
                                "text": "The designer can then repeat this process on their edited mockup.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s7",
                                "text": "To improve the LLM's performance and adapt to individual preferences, designers can provide feedback on each generated suggestion, which is integrated into the model for the next round of evaluation.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s8",
                                "text": "The plugin produces UI mockup feedback by querying an LLM with the guidelines\u2019 text and a JSON representation of the UI.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s9",
                                "text": "The LLM then returns a set of detected guideline violations.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s10",
                                "text": "Instead of directly stating the violations, they are phrased as constructive suggestions for improving the UI.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s11",
                                "text": "As LLMs can only process text and have a limited context window, we developed a JSON representation of the UI that concisely captures the layout hierarchy and contains both semantic (text, semantic label, element type) and visual (location, size, and color) details of each element and group in the UI.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p4_s12",
                                "text": "To further accommodate context window limits, we scoped the plugin to evaluate only static (i.e., non-interactive) UI mockups, one screen at a time.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "We conducted an exploration of how several current state of the art LLMs perform on this task and found that GPT-4 had the best performance by far. Hence, we solely focus on GPT-4 for the remaining studies. To assess GPT-4\u2019s performance in conducting heuristic evaluation on a large scale, we carried out a study where three design experts rated the accuracy and helpfulness of its heuristic evaluation feedback for 51 distinct UIs. To compare GPT-4\u2019s output with feedback provided by human experts, we conducted a heuristic evaluation study with 12 design experts, who manually identified guideline violations in a set of 12 UIs. Finally, to qualitatively determine GPT-4\u2019s strengths and limitations and its performance as an iterative design tool, we conducted a study with another group of 12 design experts, who each used this tool to iteratively refine a set of 3 UIs and evaluated the LLM feedback each round. For all three studies, we used diverse guidelines covering visual design, usability, and semantic organization to generate design feedback.",
                        "sentences": [
                            {
                                "id": "sec-2_p5_s1",
                                "text": "We conducted an exploration of how several current state of the art LLMs perform on this task and found that GPT-4 had the best performance by far.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p5_s2",
                                "text": "Hence, we solely focus on GPT-4 for the remaining studies.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p5_s3",
                                "text": "To assess GPT-4\u2019s performance in conducting heuristic evaluation on a large scale, we carried out a study where three design experts rated the accuracy and helpfulness of its heuristic evaluation feedback for 51 distinct UIs.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p5_s4",
                                "text": "To compare GPT-4\u2019s output with feedback provided by human experts, we conducted a heuristic evaluation study with 12 design experts, who manually identified guideline violations in a set of 12 UIs.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p5_s5",
                                "text": "Finally, to qualitatively determine GPT-4\u2019s strengths and limitations and its performance as an iterative design tool, we conducted a study with another group of 12 design experts, who each used this tool to iteratively refine a set of 3 UIs and evaluated the LLM feedback each round.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p5_s6",
                                "text": "For all three studies, we used diverse guidelines covering visual design, usability, and semantic organization to generate design feedback.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "We found that GPT-4 was generally accurate and helpful in identifying issues in poor UI designs, but its performance became worse after iterations of edits that improved the design, making it unsuitable as an iterative tool. Furthermore, its performance varied, depending on the guideline. GPT-4 generally performed well on straightforward checks with the data available in the UI JSON and worse when the JSON differed from what was visually or semantically depicted in the UI. Finally, although GPT-4\u2019s feedback is sometimes inaccurate, most study participants still found this tool useful for their own design practices, as it was able to catch subtle errors, improve the UI's text, and reason with the UI's semantics. They stated that the errors made by GPT-4 are not dangerous, as there is a human in the loop to catch them, and suggested various use cases for the tool. Finally, we also distilled a set of concrete limitations of GPT-4 for this task.",
                        "sentences": [
                            {
                                "id": "sec-2_p6_s1",
                                "text": "We found that GPT-4 was generally accurate and helpful in identifying issues in poor UI designs, but its performance became worse after iterations of edits that improved the design, making it unsuitable as an iterative tool.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p6_s2",
                                "text": "Furthermore, its performance varied, depending on the guideline.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p6_s3",
                                "text": "GPT-4 generally performed well on straightforward checks with the data available in the UI JSON and worse when the JSON differed from what was visually or semantically depicted in the UI.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p6_s4",
                                "text": "Finally, although GPT-4\u2019s feedback is sometimes inaccurate, most study participants still found this tool useful for their own design practices, as it was able to catch subtle errors, improve the UI's text, and reason with the UI's semantics.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p6_s5",
                                "text": "They stated that the errors made by GPT-4 are not dangerous, as there is a human in the loop to catch them, and suggested various use cases for the tool.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p6_s6",
                                "text": "Finally, we also distilled a set of concrete limitations of GPT-4 for this task.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "In summary, even with today's limitations, GPT-4 can already be used to automatically evaluate some heuristics for UI design; other heuristics may require more visual information or other technical advancements. However, designers accepted occasional imperfect suggestions and appreciated GPT-4\u2019s attention to detail. This implies that while LLM tools will not replace human heuristic evaluation, they may nevertheless soon find a place in design practice.",
                        "sentences": [
                            {
                                "id": "sec-2_p7_s1",
                                "text": "In summary, even with today's limitations, GPT-4 can already be used to automatically evaluate some heuristics for UI design; other heuristics may require more visual information or other technical advancements.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p7_s2",
                                "text": "However, designers accepted occasional imperfect suggestions and appreciated GPT-4\u2019s attention to detail.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            },
                            {
                                "id": "sec-2_p7_s3",
                                "text": "This implies that while LLM tools will not replace human heuristic evaluation, they may nevertheless soon find a place in design practice.",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "full_text": "Our contributions are as follows:",
                        "sentences": [
                            {
                                "id": "sec-2_p8_s1",
                                "text": "Our contributions are as follows:",
                                "context": "Section 1: INTRODUCTION",
                                "associated_visual": "fig1"
                            }
                        ]
                    }
                ],
                "lists": [
                    {
                        "id": "list_bf0c5d60",
                        "full_text": "A Figma plugin that uses GPT-4 to automate heuristic evaluation of UI mockups with arbitrary design guidelines.",
                        "type": "ul",
                        "value": "1",
                        "sentences": [
                            {
                                "id": "list_bf0c5d60_s1",
                                "text": "A Figma plugin that uses GPT-4 to automate heuristic evaluation of UI mockups with arbitrary design guidelines.",
                                "context": "List Item",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "id": "list_920c3e10",
                        "full_text": "An investigation of GPT-4\u2019s capability to automate heuristic evaluations through a study where three human participants rated the accuracy and helpfulness of LLM-generated design suggestions for 51 UIs.",
                        "type": "ul",
                        "value": "2",
                        "sentences": [
                            {
                                "id": "list_920c3e10_s1",
                                "text": "An investigation of GPT-4\u2019s capability to automate heuristic evaluations through a study where three human participants rated the accuracy and helpfulness of LLM-generated design suggestions for 51 UIs.",
                                "context": "List Item",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "id": "list_1894eeb9",
                        "full_text": "A comparison of the violations found by this tool with those identified by human experts.",
                        "type": "ul",
                        "value": "3",
                        "sentences": [
                            {
                                "id": "list_1894eeb9_s1",
                                "text": "A comparison of the violations found by this tool with those identified by human experts.",
                                "context": "List Item",
                                "associated_visual": "fig1"
                            }
                        ]
                    },
                    {
                        "id": "list_ab3ef591",
                        "full_text": "An exploration of how such a tool can fit into existing design practice via a study where 12 design experts used this tool to iteratively refine UIs, assessed the LLM-generated feedback, and discussed their experiences working with the plugin.",
                        "type": "ul",
                        "value": "4",
                        "sentences": [
                            {
                                "id": "list_ab3ef591_s1",
                                "text": "An exploration of how such a tool can fit into existing design practice via a study where 12 design experts used this tool to iteratively refine UIs, assessed the LLM-generated feedback, and discussed their experiences working with the plugin.",
                                "context": "List Item",
                                "associated_visual": "fig1"
                            }
                        ]
                    }
                ],
                "subsections": []
            },
            {
                "id": "sec-3",
                "section_number": "2",
                "title": "RELATED WORK",
                "paragraphs": [],
                "lists": [],
                "subsections": [
                    {
                        "id": "sec-4",
                        "section_number": "2.1",
                        "title": "AI-Enhanced Design Tools",
                        "paragraphs": [
                            {
                                "full_text": "Before the widespread use of generative AI, research in AI-enhanced design tools explored a variety of model architectures to accomplish a wide range of tasks. For instance, Lee et al. built a prototyping assistance tool (GUIComp) that provides multi-faceted feedback for various stages of the prototyping process. GUIComp uses an auto-encoder to support querying UI examples for design inspiration and separate convolutional neural networks to evaluate the visual complexity of the UI prototypes and predict salient regions [23]. Other studies have utilized computer vision techniques to predict saliency in graphical designs [14] and perceived tappability [50, 52]. Deep learning models have been developed for generation [7], autocompletion [5], and optimization [11, 53, 54] of UI layouts. One limitation of these techniques is that a separate model is needed for each type of task. In addition, study participants had difficulty interpreting the feedback from these models [50] and would have liked natural language explanations of detected design issues [23]. Our work addresses both of these limitations. First, our system supports arbitrary guidelines that evaluate various aspects of the UI design as input. Furthermore, the language model uses natural language to explain each detected guideline violation.",
                                "sentences": [
                                    {
                                        "id": "sec-4_p1_s1",
                                        "text": "Before the widespread use of generative AI, research in AI-enhanced design tools explored a variety of model architectures to accomplish a wide range of tasks.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s2",
                                        "text": "For instance, Lee et al. built a prototyping assistance tool (GUIComp) that provides multi-faceted feedback for various stages of the prototyping process.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s3",
                                        "text": "GUIComp uses an auto-encoder to support querying UI examples for design inspiration and separate convolutional neural networks to evaluate the visual complexity of the UI prototypes and predict salient regions [23].",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s4",
                                        "text": "Other studies have utilized computer vision techniques to predict saliency in graphical designs [14] and perceived tappability [50, 52].",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s5",
                                        "text": "Deep learning models have been developed for generation [7], autocompletion [5], and optimization [11, 53, 54] of UI layouts.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s6",
                                        "text": "One limitation of these techniques is that a separate model is needed for each type of task.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s7",
                                        "text": "In addition, study participants had difficulty interpreting the feedback from these models [50] and would have liked natural language explanations of detected design issues [23].",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s8",
                                        "text": "Our work addresses both of these limitations.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s9",
                                        "text": "First, our system supports arbitrary guidelines that evaluate various aspects of the UI design as input.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-4_p1_s10",
                                        "text": "Furthermore, the language model uses natural language to explain each detected guideline violation.",
                                        "context": "Section 2.1: AI-Enhanced Design Tools",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-5",
                        "section_number": "2.2",
                        "title": "Applications of Generative AI in Design",
                        "paragraphs": [
                            {
                                "full_text": "The recent emergence of generative AI, such as GPT, has led to various applications in design support. Park et al. carried out two studies that employ LLMs to simulate user personas in online social settings. They used GPT-3 to generate interactions on social media platforms as testing data for these platforms [44]. They later expanded on this work to build agents that could remember, reflect on, and retrieve memories from interacting with other agents to realistically simulate large-scale social interactions [43]. H\u00e4m\u00e4l\u00e4inen et al. used GPT-3 to generate synthetic human-like responses to survey questionnaires about video game experiences [18]. Finally, Wang et al. investigated the feasibility of using LLMs to interact with UIs via natural language [56]. They developed prompting techniques for tasks like screen summarization, answering questions about the screen, generating questions about the screen, and mapping instructions to UI actions. Researchers have also begun to create design tools that use Generative AI. Lawton et al. built a system where a human and generative AI model collaborate in drawing, and ran an exploratory study on the capabilities of this tool [22]. Stylette allows users to specify design goals in natural language and uses GPT to infer relevant CSS properties [19]. Perhaps most similar to our work is a study by Petridis et al. [46], who explored using LLM prompting in creating functional LLM-based UI prototypes. Their study findings showed that LLM prompts sped up prototype creation and clarified LLM-based UI requirements, which led to the development of a Figma Plugin for automated content generation and determination of optimal frame changes. These existing studies, however, have not examined the application of LLMs as a general-purpose evaluator for mobile UIs of any category with a diverse set of heuristics.",
                                "sentences": [
                                    {
                                        "id": "sec-5_p1_s1",
                                        "text": "The recent emergence of generative AI, such as GPT, has led to various applications in design support.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s2",
                                        "text": "Park et al. carried out two studies that employ LLMs to simulate user personas in online social settings.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s3",
                                        "text": "They used GPT-3 to generate interactions on social media platforms as testing data for these platforms [44].",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s4",
                                        "text": "They later expanded on this work to build agents that could remember, reflect on, and retrieve memories from interacting with other agents to realistically simulate large-scale social interactions [43].",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s5",
                                        "text": "H\u00e4m\u00e4l\u00e4inen et al. used GPT-3 to generate synthetic human-like responses to survey questionnaires about video game experiences [18].",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s6",
                                        "text": "Finally, Wang et al. investigated the feasibility of using LLMs to interact with UIs via natural language [56].",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s7",
                                        "text": "They developed prompting techniques for tasks like screen summarization, answering questions about the screen, generating questions about the screen, and mapping instructions to UI actions.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s8",
                                        "text": "Researchers have also begun to create design tools that use Generative AI.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s9",
                                        "text": "Lawton et al. built a system where a human and generative AI model collaborate in drawing, and ran an exploratory study on the capabilities of this tool [22].",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s10",
                                        "text": "Stylette allows users to specify design goals in natural language and uses GPT to infer relevant CSS properties [19].",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s11",
                                        "text": "Perhaps most similar to our work is a study by Petridis et al. [46], who explored using LLM prompting in creating functional LLM-based UI prototypes.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s12",
                                        "text": "Their study findings showed that LLM prompts sped up prototype creation and clarified LLM-based UI requirements, which led to the development of a Figma Plugin for automated content generation and determination of optimal frame changes.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-5_p1_s13",
                                        "text": "These existing studies, however, have not examined the application of LLMs as a general-purpose evaluator for mobile UIs of any category with a diverse set of heuristics.",
                                        "context": "Section 2.2: Applications of Generative AI in Design",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-6",
                        "section_number": "2.3",
                        "title": "AI-enhanced Software Testing",
                        "paragraphs": [
                            {
                                "full_text": "Another domain of UI evaluation is testing the functionality of the GUI (i.e., \u201csoftware testing\u201d). Existing LLM-based approaches include Liu et al.\u2019s method [27], which uses GPT-3 to simulate a human tester that would interact with the GUI. Their system had greater coverage and found more bugs than existing baselines, and also identified new bugs on Google Play Store apps. Wang et al. conducted a comprehensive literature review on using LLMs for software testing. They analyzed various studies that used LLMs for unit test generation, validation of test outputs, test input generation, analyzing bugs, fixing identified bugs in code, and identifying and correcting bugs. Contrary to software testing, our study focuses on evaluating GUI mockups, which is at an earlier stage of the UI development process. Furthermore, evaluation of mockups and software are intrinsically different; mockup evaluation focuses on adherence to design guidelines and user feedback, whereas software testing focuses on finding bugs in the implementation. Prior to LLMs, Chen et al. utilized computer vision techniques to identify discrepancies between the UI mockup and implementation [6]. Their system could identify differences in positioning, color, and size of corresponding elements. However, their evaluation requires a UI mockup as the benchmark, while our system could carry out evaluation using any set of heuristics.",
                                "sentences": [
                                    {
                                        "id": "sec-6_p1_s1",
                                        "text": "Another domain of UI evaluation is testing the functionality of the GUI (i.e., \u201csoftware testing\u201d).",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s2",
                                        "text": "Existing LLM-based approaches include Liu et al.\u2019s method [27], which uses GPT-3 to simulate a human tester that would interact with the GUI.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s3",
                                        "text": "Their system had greater coverage and found more bugs than existing baselines, and also identified new bugs on Google Play Store apps.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s4",
                                        "text": "Wang et al. conducted a comprehensive literature review on using LLMs for software testing.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s5",
                                        "text": "They analyzed various studies that used LLMs for unit test generation, validation of test outputs, test input generation, analyzing bugs, fixing identified bugs in code, and identifying and correcting bugs.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s6",
                                        "text": "Contrary to software testing, our study focuses on evaluating GUI mockups, which is at an earlier stage of the UI development process.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s7",
                                        "text": "Furthermore, evaluation of mockups and software are intrinsically different; mockup evaluation focuses on adherence to design guidelines and user feedback, whereas software testing focuses on finding bugs in the implementation.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s8",
                                        "text": "Prior to LLMs, Chen et al. utilized computer vision techniques to identify discrepancies between the UI mockup and implementation [6].",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s9",
                                        "text": "Their system could identify differences in positioning, color, and size of corresponding elements.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-6_p1_s10",
                                        "text": "However, their evaluation requires a UI mockup as the benchmark, while our system could carry out evaluation using any set of heuristics.",
                                        "context": "Section 2.3: AI-enhanced Software Testing",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-7",
                        "section_number": "2.4",
                        "title": "Heuristics and Design Guidelines",
                        "paragraphs": [
                            {
                                "full_text": "An essential aspect of the design process is gathering feedback to improve future iterations. One central way designers generate feedback is to conduct heuristic evaluations [38, 40], which uses a set of guidelines to identify and characterize undesired interface characteristics as violations of specific guidelines. While initially designed for desktop interfaces, other work has adapted heuristic evaluation to more devices and domains [9, 31, 33, 47]. In general, researchers have developed design guidelines for a vast category of devices, tasks, and populations including accessible data visualizations [12], multi-modal touchscreen graphics [17], smart televisions [58], ambient lighting interactions [32], hands-free speech interaction [35], navigation in virtual environments [55], website readability [34], supporting web design for aging communities [21, 61], and for cross-cultural design considerations [2]. A widely-used set of guidelines is Nielsen's 10 Usability Heuristics [39], a set of general principles for interaction design. Luther et al. surveyed design textbooks and other resources and compiled a comprehensive set of specific critique statements for the visual design of an interface, which were organized into 7 visual design principles [29]. Recently, Duan et al. developed a set of 5 specific and actionable guidelines for organizing UI elements based on their semantics (i.e., functionality, content, or purpose) to help design clear and intuitive interfaces [10]. While these guidelines are meant to encode common design patterns and errors distilled from design expert guidance, they still require a human to interpret and apply them, making adapting to a new set of guidelines time-consuming, especially for novice designers. Our work builds off of these design guidelines as a means of focusing and justifying the LLM's design suggestions and feedback.",
                                "sentences": [
                                    {
                                        "id": "sec-7_p1_s1",
                                        "text": "An essential aspect of the design process is gathering feedback to improve future iterations.",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s2",
                                        "text": "One central way designers generate feedback is to conduct heuristic evaluations [38, 40], which uses a set of guidelines to identify and characterize undesired interface characteristics as violations of specific guidelines.",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s3",
                                        "text": "While initially designed for desktop interfaces, other work has adapted heuristic evaluation to more devices and domains [9, 31, 33, 47].",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s4",
                                        "text": "In general, researchers have developed design guidelines for a vast category of devices, tasks, and populations including accessible data visualizations [12], multi-modal touchscreen graphics [17], smart televisions [58], ambient lighting interactions [32], hands-free speech interaction [35], navigation in virtual environments [55], website readability [34], supporting web design for aging communities [21, 61], and for cross-cultural design considerations [2].",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s5",
                                        "text": "A widely-used set of guidelines is Nielsen's 10 Usability Heuristics [39], a set of general principles for interaction design.",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s6",
                                        "text": "Luther et al. surveyed design textbooks and other resources and compiled a comprehensive set of specific critique statements for the visual design of an interface, which were organized into 7 visual design principles [29].",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s7",
                                        "text": "Recently, Duan et al. developed a set of 5 specific and actionable guidelines for organizing UI elements based on their semantics (i.e., functionality, content, or purpose) to help design clear and intuitive interfaces [10].",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s8",
                                        "text": "While these guidelines are meant to encode common design patterns and errors distilled from design expert guidance, they still require a human to interpret and apply them, making adapting to a new set of guidelines time-consuming, especially for novice designers.",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-7_p1_s9",
                                        "text": "Our work builds off of these design guidelines as a means of focusing and justifying the LLM's design suggestions and feedback.",
                                        "context": "Section 2.4: Heuristics and Design Guidelines",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-8",
                        "section_number": "2.5",
                        "title": "User Interfaces for Design Feedback",
                        "paragraphs": [
                            {
                                "full_text": "Prior research has explored several ways to support designers as they both give and receive feedback across a range of media [20, 30, 45, 57, 60]. Cheng et al. explore the process of publicly gathering design feedback from online forums [8] and list several design considerations for feedback systems. For supporting in-context feedback for graphic designs, CritiqueKit [15] showcased a UI for providing and improving real-time design feedback, while Charrette [41] supported organizing and sharing feedback on longer histories and variations of a design. A study by Ngoon et al. showcased reusing expert feedback suggestions and adaptive guidance as two ways of improving creative feedback by making the feedback more specific, justified, and actionable [36]. This notion of adaptive conceptual guidance is further explored by Sh\u00f6wn [37], demonstrating the utility of adapting presented design suggestions and examples automatically given the user's current working context. Our plugin provides in-context design feedback grounded by this prior work on user interfaces for design feedback, while automatically generating the provided feedback and design suggestions.",
                                "sentences": [
                                    {
                                        "id": "sec-8_p1_s1",
                                        "text": "Prior research has explored several ways to support designers as they both give and receive feedback across a range of media [20, 30, 45, 57, 60].",
                                        "context": "Section 2.5: User Interfaces for Design Feedback",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-8_p1_s2",
                                        "text": "Cheng et al. explore the process of publicly gathering design feedback from online forums [8] and list several design considerations for feedback systems.",
                                        "context": "Section 2.5: User Interfaces for Design Feedback",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-8_p1_s3",
                                        "text": "For supporting in-context feedback for graphic designs, CritiqueKit [15] showcased a UI for providing and improving real-time design feedback, while Charrette [41] supported organizing and sharing feedback on longer histories and variations of a design.",
                                        "context": "Section 2.5: User Interfaces for Design Feedback",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-8_p1_s4",
                                        "text": "A study by Ngoon et al. showcased reusing expert feedback suggestions and adaptive guidance as two ways of improving creative feedback by making the feedback more specific, justified, and actionable [36].",
                                        "context": "Section 2.5: User Interfaces for Design Feedback",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-8_p1_s5",
                                        "text": "This notion of adaptive conceptual guidance is further explored by Sh\u00f6wn [37], demonstrating the utility of adapting presented design suggestions and examples automatically given the user's current working context.",
                                        "context": "Section 2.5: User Interfaces for Design Feedback",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-8_p1_s6",
                                        "text": "Our plugin provides in-context design feedback grounded by this prior work on user interfaces for design feedback, while automatically generating the provided feedback and design suggestions.",
                                        "context": "Section 2.5: User Interfaces for Design Feedback",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    }
                ]
            },
            {
                "id": "sec-9",
                "section_number": "3",
                "title": "SYSTEM DETAILS",
                "paragraphs": [
                    {
                        "full_text": "In this section, we describe the set of design goals for an automatic LLM-driven heuristic evaluation tool, how they are realized in our system, the underlying implementation, techniques to improve the LLM's performance, and explorations of alternative prompt designs and various LLM models for this task.",
                        "sentences": [
                            {
                                "id": "sec-9_p1_s1",
                                "text": "In this section, we describe the set of design goals for an automatic LLM-driven heuristic evaluation tool, how they are realized in our system, the underlying implementation, techniques to improve the LLM's performance, and explorations of alternative prompt designs and various LLM models for this task.",
                                "context": "Section 3: SYSTEM DETAILS",
                                "associated_visual": "fig1"
                            }
                        ]
                    }
                ],
                "lists": [],
                "subsections": [
                    {
                        "id": "sec-10",
                        "section_number": "3.1",
                        "title": "Design Goals",
                        "paragraphs": [
                            {
                                "full_text": "Based on design principles and expected LLM behavior, we came up with a set of goals that lay out what an automatic LLM-based heuristic evaluation tool should be able to do. The goals are as follows:",
                                "sentences": [
                                    {
                                        "id": "sec-10_p1_s1",
                                        "text": "Based on design principles and expected LLM behavior, we came up with a set of goals that lay out what an automatic LLM-based heuristic evaluation tool should be able to do.",
                                        "context": "Section 3.1: Design Goals",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-10_p1_s2",
                                        "text": "The goals are as follows:",
                                        "context": "Section 3.1: Design Goals",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "lists": [
                            {
                                "id": "list_6dff72ed",
                                "full_text": "The tool should be able to accommodate arbitrary UI prototypes; designers should be able to use this tool to perform heuristic evaluations on their mockups and identify potential issues, before implementation.",
                                "type": "ol",
                                "value": "1",
                                "sentences": [
                                    {
                                        "id": "list_6dff72ed_s1",
                                        "text": "The tool should be able to accommodate arbitrary UI prototypes; designers should be able to use this tool to perform heuristic evaluations on their mockups and identify potential issues, before implementation.",
                                        "context": "List Item",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            },
                            {
                                "id": "list_475f5d7d",
                                "full_text": "The tool should be heuristic-agnostic, so different guidelines or heuristics can be used.",
                                "type": "ol",
                                "value": "2",
                                "sentences": [
                                    {
                                        "id": "list_475f5d7d_s1",
                                        "text": "The tool should be heuristic-agnostic, so different guidelines or heuristics can be used.",
                                        "context": "List Item",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            },
                            {
                                "id": "list_b7461e05",
                                "full_text": "The guideline violations detected by the LLM should be presented in a way that adheres to the principles of effective feedback [48].",
                                "type": "ol",
                                "value": "3",
                                "sentences": [
                                    {
                                        "id": "list_b7461e05_s1",
                                        "text": "The guideline violations detected by the LLM should be presented in a way that adheres to the principles of effective feedback [48].",
                                        "context": "List Item",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            },
                            {
                                "id": "list_57d3e9bd",
                                "full_text": "The LLM generated feedback should be presented in the context of the critiqued design. This is to narrow the gulf of evaluation, making it easier for designers to interpret the feedback.",
                                "type": "ol",
                                "value": "4",
                                "sentences": [
                                    {
                                        "id": "list_57d3e9bd_s1",
                                        "text": "The LLM generated feedback should be presented in the context of the critiqued design. This is to narrow the gulf of evaluation, making it easier for designers to interpret the feedback.",
                                        "context": "List Item",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            },
                            {
                                "id": "list_83faac10",
                                "full_text": "Finally, in case the LLM makes a mistake, the designer should be able to hide feedback they find unhelpful. This data should also be sent to the LLM to improve its prediction accuracy.",
                                "type": "ol",
                                "value": "5",
                                "sentences": [
                                    {
                                        "id": "list_83faac10_s1",
                                        "text": "Finally, in case the LLM makes a mistake, the designer should be able to hide feedback they find unhelpful. This data should also be sent to the LLM to improve its prediction accuracy.",
                                        "context": "List Item",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            }
                        ],
                        "subsections": []
                    },
                    {
                        "id": "sec-11",
                        "section_number": "3.2",
                        "title": "Design Walkthrough",
                        "paragraphs": [
                            {
                                "full_text": "We built our tool as a plugin for Figma, enabling designers to evaluate any Figma mockup (Goal 1). Figure 1 illustrates this plugin's step-by-step usage with interface screenshots. The designer first prototypes their UI in Figma and runs the plugin (Figure 1 Box A). Due to context window limitations, the plugin only evaluates a single UI screen at a time. Furthermore, it only assesses static mockups, as evaluation of interactive mockups may require multiple screens as input or more complex UI representations, which could exceed the LLM's context limit. After starting the plugin, it opens up a page to select guidelines to use for heuristic evaluation (Box B). Designers can select from a set of well-known guidelines, like Nielsen's 10 Usability Heuristics, or enter any list of heuristics they would like to use (Goal 2). They can also select more than one set of guidelines for the evaluation.",
                                "sentences": [
                                    {
                                        "id": "sec-11_p1_s1",
                                        "text": "We built our tool as a plugin for Figma, enabling designers to evaluate any Figma mockup (Goal 1).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s2",
                                        "text": "Figure 1 illustrates this plugin's step-by-step usage with interface screenshots.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s3",
                                        "text": "The designer first prototypes their UI in Figma and runs the plugin (Figure 1 Box A).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s4",
                                        "text": "Due to context window limitations, the plugin only evaluates a single UI screen at a time.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s5",
                                        "text": "Furthermore, it only assesses static mockups, as evaluation of interactive mockups may require multiple screens as input or more complex UI representations, which could exceed the LLM's context limit.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s6",
                                        "text": "After starting the plugin, it opens up a page to select guidelines to use for heuristic evaluation (Box B).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s7",
                                        "text": "Designers can select from a set of well-known guidelines, like Nielsen's 10 Usability Heuristics, or enter any list of heuristics they would like to use (Goal 2).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p1_s8",
                                        "text": "They can also select more than one set of guidelines for the evaluation.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            },
                            {
                                "full_text": "Once the LLM completes the heuristic evaluation, text explanations of all violations found and a UI screenshot are rendered back to the designer (Figure 1 Box C). This \u201cUI Snapshot\u201d serves as a reference to the state of the mockup at the time of evaluation, in case the designer makes any changes based on the evaluation results. Each violation explanation contains the name of the violated guideline and is phrased as constructive feedback, following the guidelines set by Sadler et al. [48] (Goal 3). According to Sadler, effective feedback is specific and relevant, highlighting the performance gap and providing actionable guidance for improvement. To accomplish this, the feedback must include these three things: 1) the expected standard, 2) the gap between the quality of work and the standard, and 3) what needs to be done to close this gap. Our design feedback adheres to Sadler's principles and starts by stating the standard set by the guideline, followed by the issue with the current design (the gap between the design and expected standard), and concludes with advice on fixing the issue. Figure 9 provides four examples of these explanations.",
                                "sentences": [
                                    {
                                        "id": "sec-11_p2_s1",
                                        "text": "Once the LLM completes the heuristic evaluation, text explanations of all violations found and a UI screenshot are rendered back to the designer (Figure 1 Box C).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p2_s2",
                                        "text": "This \u201cUI Snapshot\u201d serves as a reference to the state of the mockup at the time of evaluation, in case the designer makes any changes based on the evaluation results.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p2_s3",
                                        "text": "Each violation explanation contains the name of the violated guideline and is phrased as constructive feedback, following the guidelines set by Sadler et al. [48] (Goal 3).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p2_s4",
                                        "text": "According to Sadler, effective feedback is specific and relevant, highlighting the performance gap and providing actionable guidance for improvement.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p2_s5",
                                        "text": "To accomplish this, the feedback must include these three things: 1) the expected standard, 2) the gap between the quality of work and the standard, and 3) what needs to be done to close this gap.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p2_s6",
                                        "text": "Our design feedback adheres to Sadler's principles and starts by stating the standard set by the guideline, followed by the issue with the current design (the gap between the design and expected standard), and concludes with advice on fixing the issue.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    },
                                    {
                                        "id": "sec-11_p2_s7",
                                        "text": "Figure 9 provides four examples of these explanations.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig1"
                                    }
                                ]
                            },
                            {
                                "full_text": "The plugin also includes several features that help designers contextualize the text feedback with corresponding UI elements. (Goal 4). Figure 2 illustrates these features. Selecting a violation fades the other suggestions and draws a box around the relevant group or element in the screenshot, as shown in Figure 2 (B). In addition, all UI elements and groups mentioned are rendered as links. Hovering over a link draws a box over the corresponding group or element in the screenshot (C), and clicking on the link selects the item in the Figma mockup and Layers panel (A), streamlining the editing process. Finally, to address Goal 5, if the designer finds a suggestion incorrect or unhelpful, they can click on its \u2018X\u2019 icon to hide it (D). Hiding the violation sends feedback to the LLM for subsequent evaluation rounds so this violation will not be shown again.",
                                "sentences": [
                                    {
                                        "id": "sec-11_p3_s1",
                                        "text": "The plugin also includes several features that help designers contextualize the text feedback with corresponding UI elements. (Goal 4).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p3_s2",
                                        "text": "Figure 2 illustrates these features.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p3_s3",
                                        "text": "Selecting a violation fades the other suggestions and draws a box around the relevant group or element in the screenshot, as shown in Figure 2 (B).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p3_s4",
                                        "text": "In addition, all UI elements and groups mentioned are rendered as links.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p3_s5",
                                        "text": "Hovering over a link draws a box over the corresponding group or element in the screenshot (C), and clicking on the link selects the item in the Figma mockup and Layers panel (A), streamlining the editing process.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p3_s6",
                                        "text": "Finally, to address Goal 5, if the designer finds a suggestion incorrect or unhelpful, they can click on its \u2018X\u2019 icon to hide it (D).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p3_s7",
                                        "text": "Hiding the violation sends feedback to the LLM for subsequent evaluation rounds so this violation will not be shown again.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    }
                                ]
                            },
                            {
                                "full_text": "After the designer revises their mockup based on LLM feedback, they can rerun the evaluation to generate new suggestions. This usage is intended to match the iterative feedback and revision process during design. The plugin uses the information from the Layers panel of Figma to create the text-based representation of the mockup (discussed in more detail in the next section). Hence, it relies on accurate names for groups in the Layers panel to convey semantic information about the UI; for instance, the group containing icons in the navbar should be named \u201cnavbar\u201d. Designers must manually add these names, so they are often missing. To address this, we implemented an auxiliary label generation feature that can be run before evaluation to generate group names automatically (based on their contents).",
                                "sentences": [
                                    {
                                        "id": "sec-11_p4_s1",
                                        "text": "After the designer revises their mockup based on LLM feedback, they can rerun the evaluation to generate new suggestions.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p4_s2",
                                        "text": "This usage is intended to match the iterative feedback and revision process during design.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p4_s3",
                                        "text": "The plugin uses the information from the Layers panel of Figma to create the text-based representation of the mockup (discussed in more detail in the next section).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p4_s4",
                                        "text": "Hence, it relies on accurate names for groups in the Layers panel to convey semantic information about the UI; for instance, the group containing icons in the navbar should be named \u201cnavbar\u201d.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p4_s5",
                                        "text": "Designers must manually add these names, so they are often missing.",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    },
                                    {
                                        "id": "sec-11_p4_s6",
                                        "text": "To address this, we implemented an auxiliary label generation feature that can be run before evaluation to generate group names automatically (based on their contents).",
                                        "context": "Section 3.2: Design Walkthrough",
                                        "associated_visual": "fig2"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-12",
                        "section_number": "3.3",
                        "title": "Implementation",
                        "paragraphs": [
                            {
                                "full_text": "We implemented this plugin in Typescript using the Figma Plugin API. The plugin makes an API request to OpenAI's GPT-4 for LLM queries. Since LLMs can only accept text as input, the plugin takes in a JSON representation of the UI. While multi-modal models exist that could take in both the UI screenshot and guidelines text (e.g., [26]), we found that its performance was considerably worse than GPT-4\u2019s for this task (at the time).",
                                "sentences": [
                                    {
                                        "id": "sec-12_p1_s1",
                                        "text": "We implemented this plugin in Typescript using the Figma Plugin API.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p1_s2",
                                        "text": "The plugin makes an API request to OpenAI's GPT-4 for LLM queries.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p1_s3",
                                        "text": "Since LLMs can only accept text as input, the plugin takes in a JSON representation of the UI.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p1_s4",
                                        "text": "While multi-modal models exist that could take in both the UI screenshot and guidelines text (e.g., [26]), we found that its performance was considerably worse than GPT-4\u2019s for this task (at the time).",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    }
                                ]
                            },
                            {
                                "full_text": "Our JSON format captures the DOM (Document Object Model) structure of the UI mockup, and is similar in structure and content to the HTML-based representation used by [56] that performed well on UI-related tasks. Figure 4 contains an example portion of a UI JSON with corresponding groups and elements marked in the UI screenshot. The tree structure is informative of the overall organization of the UI, with UI elements (buttons, icons, etc.) as leaves and groups (of elements and/or smaller groups) as intermediate nodes. Each node in this JSON tree contains semantic information (text labels, element or group names, and element type) and visual data (x,y-position of the top left corner, height, width, color, opacity, background color, font, etc.) of its element or group. Hence, this JSON representation captures both semantic and visual features of the UI, which supports the evaluation of various aspects of the design and differentiates it from the representation used by [56] that captures only semantic information. This JSON representation is constructed from the data (e.g., group/element names) and grouping structure found in the Layers panel of Figma, which are editable by designers.",
                                "sentences": [
                                    {
                                        "id": "sec-12_p2_s1",
                                        "text": "Our JSON format captures the DOM (Document Object Model) structure of the UI mockup, and is similar in structure and content to the HTML-based representation used by [56] that performed well on UI-related tasks.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p2_s2",
                                        "text": "Figure 4 contains an example portion of a UI JSON with corresponding groups and elements marked in the UI screenshot.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p2_s3",
                                        "text": "The tree structure is informative of the overall organization of the UI, with UI elements (buttons, icons, etc.) as leaves and groups (of elements and/or smaller groups) as intermediate nodes.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p2_s4",
                                        "text": "Each node in this JSON tree contains semantic information (text labels, element or group names, and element type) and visual data (x,y-position of the top left corner, height, width, color, opacity, background color, font, etc.) of its element or group.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p2_s5",
                                        "text": "Hence, this JSON representation captures both semantic and visual features of the UI, which supports the evaluation of various aspects of the design and differentiates it from the representation used by [56] that captures only semantic information.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    },
                                    {
                                        "id": "sec-12_p2_s6",
                                        "text": "This JSON representation is constructed from the data (e.g., group/element names) and grouping structure found in the Layers panel of Figma, which are editable by designers.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig4"
                                    }
                                ]
                            },
                            {
                                "full_text": "Figure 3 shows the core system design of the plugin. Due to the context window limits of GPT-4, we remove all unnecessary or redundant information and condense verbose details into a concise JSON structure (Box 3). This condensed JSON representation and guideline text are combined into a prompt sent to the LLM. After the LLM returns the identified guideline violations, another query is sent to the LLM to convert these violations into constructive advice (Box 4). This chain of prompts is illustrated in Figure 12 (Appendix), which describes the components of each prompt. The LLM response is parsed by the TypeScript code (Box 5) and rendered into an interpretable format for designers (Box 6). Figma IDs for each element and group are stored internally, which supports selection of elements or groups in the mockup via links (Figure 2, A) and quick access to their layout information. Layout information is used to draw boxes around elements and groups in the screenshot, as shown in Figure 2 (B and C). Finally, unhelpful suggestions that were dismissed by the designer are incorporated into the prompt for the next round of evaluation (Box 7), if there is room in the context window. The label generation feature is also executed via an LLM call, with the prompt containing JSON data of all unnamed groups and instructions for the LLM to create a descriptive label for each JSON based on its contents.",
                                "sentences": [
                                    {
                                        "id": "sec-12_p3_s1",
                                        "text": "Figure 3 shows the core system design of the plugin.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s2",
                                        "text": "Due to the context window limits of GPT-4, we remove all unnecessary or redundant information and condense verbose details into a concise JSON structure (Box 3).",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s3",
                                        "text": "This condensed JSON representation and guideline text are combined into a prompt sent to the LLM.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s4",
                                        "text": "After the LLM returns the identified guideline violations, another query is sent to the LLM to convert these violations into constructive advice (Box 4).",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s5",
                                        "text": "This chain of prompts is illustrated in Figure 12 (Appendix), which describes the components of each prompt.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s6",
                                        "text": "The LLM response is parsed by the TypeScript code (Box 5) and rendered into an interpretable format for designers (Box 6).",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s7",
                                        "text": "Figma IDs for each element and group are stored internally, which supports selection of elements or groups in the mockup via links (Figure 2, A) and quick access to their layout information.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s8",
                                        "text": "Layout information is used to draw boxes around elements and groups in the screenshot, as shown in Figure 2 (B and C).",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s9",
                                        "text": "Finally, unhelpful suggestions that were dismissed by the designer are incorporated into the prompt for the next round of evaluation (Box 7), if there is room in the context window.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    },
                                    {
                                        "id": "sec-12_p3_s10",
                                        "text": "The label generation feature is also executed via an LLM call, with the prompt containing JSON data of all unnamed groups and instructions for the LLM to create a descriptive label for each JSON based on its contents.",
                                        "context": "Section 3.3: Implementation",
                                        "associated_visual": "fig3"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-13",
                        "section_number": "3.4",
                        "title": "Improving LLM Performance",
                        "paragraphs": [
                            {
                                "full_text": "We chose the most advanced GPT version available (GPT-4), as it has the strongest reasoning abilities [42]. However, GPT-4 does not support fine-tuning and has a context window limit of 8.1k tokens. This context window limit leaves inadequate room for few-shot and \u201cchain-of-thought\u201d [59] examples because each Figma UI JSON requires around 3-5k tokens, the guidelines text take up to 2k tokens, and few-shot and chain-of-thought examples both require the corresponding UI JSONs. Due to these limitations, our method for improving GPT-4\u2019s performance entailed adding explicit instructions in the prompt to avoid common mistakes, as shown in Figure 12 (Appendix). Finally, we set the temperature to 0 to ensure GPT-4 returns the most probable violations.",
                                "sentences": [
                                    {
                                        "id": "sec-13_p1_s1",
                                        "text": "We chose the most advanced GPT version available (GPT-4), as it has the strongest reasoning abilities [42].",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p1_s2",
                                        "text": "However, GPT-4 does not support fine-tuning and has a context window limit of 8.1k tokens.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p1_s3",
                                        "text": "This context window limit leaves inadequate room for few-shot and \u201cchain-of-thought\u201d [59] examples because each Figma UI JSON requires around 3-5k tokens, the guidelines text take up to 2k tokens, and few-shot and chain-of-thought examples both require the corresponding UI JSONs.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p1_s4",
                                        "text": "Due to these limitations, our method for improving GPT-4\u2019s performance entailed adding explicit instructions in the prompt to avoid common mistakes, as shown in Figure 12 (Appendix).",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p1_s5",
                                        "text": "Finally, we set the temperature to 0 to ensure GPT-4 returns the most probable violations.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    }
                                ]
                            },
                            {
                                "full_text": "The remaining space in the context window was allocated to suggestions that were dismissed (hidden) by designers. Incorporating this feedback targets areas of poor performance specific to the UI being evaluated and also adapts GPT-4\u2019s feedback to the designer's preferences. Since the UI JSON is already provided in the prompt, this feedback does not require much space. However, the UI may have changed due to edits, so JSONs of the groups/elements for a dismissed violation are still included, but they are considerably smaller than the entire UI JSON. These items are incorporated in the conversation history of the next prompt, as examples of inaccurate suggestions (see Figure 12 in the Appendix). In addition, we ask GPT-4 to reflect on why it was wrong and add this prompt and its response to the conversation history. This \u201cself-reflection\u201d has been shown to improve LLM performance [51].",
                                "sentences": [
                                    {
                                        "id": "sec-13_p2_s1",
                                        "text": "The remaining space in the context window was allocated to suggestions that were dismissed (hidden) by designers.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p2_s2",
                                        "text": "Incorporating this feedback targets areas of poor performance specific to the UI being evaluated and also adapts GPT-4\u2019s feedback to the designer's preferences.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p2_s3",
                                        "text": "Since the UI JSON is already provided in the prompt, this feedback does not require much space.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p2_s4",
                                        "text": "However, the UI may have changed due to edits, so JSONs of the groups/elements for a dismissed violation are still included, but they are considerably smaller than the entire UI JSON.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p2_s5",
                                        "text": "These items are incorporated in the conversation history of the next prompt, as examples of inaccurate suggestions (see Figure 12 in the Appendix).",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p2_s6",
                                        "text": "In addition, we ask GPT-4 to reflect on why it was wrong and add this prompt and its response to the conversation history.",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    },
                                    {
                                        "id": "sec-13_p2_s7",
                                        "text": "This \u201cself-reflection\u201d has been shown to improve LLM performance [51].",
                                        "context": "Section 3.4: Improving LLM Performance",
                                        "associated_visual": "fig12"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-14",
                        "section_number": "3.5",
                        "title": "Exploration of Alternative Prompt Compositions",
                        "paragraphs": [
                            {
                                "full_text": "We investigate how different prompt components influence GPT-4\u2019s output to identify potential opportunities for simplifying our complex prompt. For our analysis, we used 12 distinct mockups of mobile UIs taken from the Figma community. Furthermore, we used three sets of heuristics covering different aspects of UI design: Nielsen's 10 Usability Heuristics [38], Luther et al.\u2019s visual design principles compiled in \u201cCrowdCrit\u201d [29], and Duan et al.\u2019s 5 semantic grouping guidelines [10]. These 12 UIs and three sets of heuristics were consistently used in all subsequent analyses and studies in this paper, except for the Performance Study, which used a larger set of 51 UIs. We query the LLM with prompt variations and then compute the total number of reported violations and the number of helpful violations (based on the authors\u2019 judgment), and we also qualitatively examine the violations. We consider a violation to be helpful if it is both accurate and would lead to an improvement in the design. Table 3.5 (\u201cPrompt Condition\u201d) compares violation counts for each condition with the complete prompt chain.",
                                "sentences": [
                                    {
                                        "id": "sec-14_p1_s1",
                                        "text": "We investigate how different prompt components influence GPT-4\u2019s output to identify potential opportunities for simplifying our complex prompt.",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-14_p1_s2",
                                        "text": "For our analysis, we used 12 distinct mockups of mobile UIs taken from the Figma community.",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-14_p1_s3",
                                        "text": "Furthermore, we used three sets of heuristics covering different aspects of UI design: Nielsen's 10 Usability Heuristics [38], Luther et al.\u2019s visual design principles compiled in \u201cCrowdCrit\u201d [29], and Duan et al.\u2019s 5 semantic grouping guidelines [10].",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-14_p1_s4",
                                        "text": "These 12 UIs and three sets of heuristics were consistently used in all subsequent analyses and studies in this paper, except for the Performance Study, which used a larger set of 51 UIs.",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-14_p1_s5",
                                        "text": "We query the LLM with prompt variations and then compute the total number of reported violations and the number of helpful violations (based on the authors\u2019 judgment), and we also qualitatively examine the violations.",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-14_p1_s6",
                                        "text": "We consider a violation to be helpful if it is both accurate and would lead to an improvement in the design.",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-14_p1_s7",
                                        "text": "Table 3.5 (\u201cPrompt Condition\u201d) compares violation counts for each condition with the complete prompt chain.",
                                        "context": "Section 3.5: Exploration of Alternative Prompt Compositions",
                                        "associated_visual": "table1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": [
                            {
                                "id": "sec-15",
                                "section_number": "3.5.1",
                                "title": "One Call",
                                "paragraphs": [
                                    {
                                        "full_text": "Our prompt chain makes two LLM calls \u2013 one to carry out the heuristic evaluation and the other to rephrase results into constructive feedback (Appendix Figure 12). We examine the effects of combining these two into a single call, as this would reduce latency. Quantitatively, the total number of violations remained similar, but the number of helpful violations was lower. However, more importantly, the output was never formatted correctly with one call, and the format also varied across different calls. Furthermore, GPT-4 sometimes omitted other important details, such as how to fix the violation. Since correct output formatting is necessary for the plugin to parse and render the violations, combining the two calls is not feasible.",
                                        "sentences": [
                                            {
                                                "id": "sec-15_p1_s1",
                                                "text": "Our prompt chain makes two LLM calls \u2013 one to carry out the heuristic evaluation and the other to rephrase results into constructive feedback (Appendix Figure 12).",
                                                "context": "Section 3.5.1: One Call",
                                                "associated_visual": "fig12"
                                            },
                                            {
                                                "id": "sec-15_p1_s2",
                                                "text": "We examine the effects of combining these two into a single call, as this would reduce latency.",
                                                "context": "Section 3.5.1: One Call",
                                                "associated_visual": "fig12"
                                            },
                                            {
                                                "id": "sec-15_p1_s3",
                                                "text": "Quantitatively, the total number of violations remained similar, but the number of helpful violations was lower.",
                                                "context": "Section 3.5.1: One Call",
                                                "associated_visual": "fig12"
                                            },
                                            {
                                                "id": "sec-15_p1_s4",
                                                "text": "However, more importantly, the output was never formatted correctly with one call, and the format also varied across different calls.",
                                                "context": "Section 3.5.1: One Call",
                                                "associated_visual": "fig12"
                                            },
                                            {
                                                "id": "sec-15_p1_s5",
                                                "text": "Furthermore, GPT-4 sometimes omitted other important details, such as how to fix the violation.",
                                                "context": "Section 3.5.1: One Call",
                                                "associated_visual": "fig12"
                                            },
                                            {
                                                "id": "sec-15_p1_s6",
                                                "text": "Since correct output formatting is necessary for the plugin to parse and render the violations, combining the two calls is not feasible.",
                                                "context": "Section 3.5.1: One Call",
                                                "associated_visual": "fig12"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-16",
                                "section_number": "3.5.2",
                                "title": "No Heuristics",
                                "paragraphs": [
                                    {
                                        "full_text": "The detailed heuristics text occupies a lot of space in the LLM's context window, so we examined the performance without including them in the prompt. We edited prompts to look for \u201cvisual design issues\u201d, \u201cusability issues\u201d, or \u201csemantic group issues\u201d instead of passing in the heuristics.",
                                        "sentences": [
                                            {
                                                "id": "sec-16_p1_s1",
                                                "text": "The detailed heuristics text occupies a lot of space in the LLM's context window, so we examined the performance without including them in the prompt.",
                                                "context": "Section 3.5.2: No Heuristics",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-16_p1_s2",
                                                "text": "We edited prompts to look for \u201cvisual design issues\u201d, \u201cusability issues\u201d, or \u201csemantic group issues\u201d instead of passing in the heuristics.",
                                                "context": "Section 3.5.2: No Heuristics",
                                                "associated_visual": "table1"
                                            }
                                        ]
                                    },
                                    {
                                        "full_text": "Table 3.5 (top) shows that GPT-4 provided fewer suggestions total (50 vs. 63) and considerably fewer helpful suggestions when heuristics were not included in the prompt (14 vs. 38). Qualitatively, the suggestions for Crowdcrit and Nielsen were similar to those from the complete prompt, though the suggestions were more thorough when the Crowdcrit heuristics were included. However, for Semantic Grouping, not passing in the heuristics resulted in only violations that concerned the semantic relatedness of group members, whereas passing in the guidelines resulted in a more diverse set of issues found. We conclude that while the LLM could give plausible UI feedback without passing in heuristics, the quality of the suggestions is worse.",
                                        "sentences": [
                                            {
                                                "id": "sec-16_p2_s1",
                                                "text": "Table 3.5 (top) shows that GPT-4 provided fewer suggestions total (50 vs. 63) and considerably fewer helpful suggestions when heuristics were not included in the prompt (14 vs. 38).",
                                                "context": "Section 3.5.2: No Heuristics",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-16_p2_s2",
                                                "text": "Qualitatively, the suggestions for Crowdcrit and Nielsen were similar to those from the complete prompt, though the suggestions were more thorough when the Crowdcrit heuristics were included.",
                                                "context": "Section 3.5.2: No Heuristics",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-16_p2_s3",
                                                "text": "However, for Semantic Grouping, not passing in the heuristics resulted in only violations that concerned the semantic relatedness of group members, whereas passing in the guidelines resulted in a more diverse set of issues found.",
                                                "context": "Section 3.5.2: No Heuristics",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-16_p2_s4",
                                                "text": "We conclude that while the LLM could give plausible UI feedback without passing in heuristics, the quality of the suggestions is worse.",
                                                "context": "Section 3.5.2: No Heuristics",
                                                "associated_visual": "table1"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-17",
                                "section_number": "3.5.3",
                                "title": "General UI Feedback",
                                "paragraphs": [
                                    {
                                        "full_text": "Finally, we investigate how GPT-4 responds without specific guidance when prompted for general UI feedback. We removed all mentions of \u201cguidelines\u201d in the prompt and replaced \u201cviolations\u201d with \u201cfeedback.\u201d Quantitatively, the performance for this condition was worse. Qualitatively, GPT-4 still carried out heuristic evaluation to an extent, as the issues were grounded in existing design conventions, but in a less rigorous and organized manner. Compared to the complete prompt, the feedback was less diverse, and the LLM often focused on only one type of issue (e.g., misalignment) when there were other types of violations. We conclude that GPT-4 can produce plausible output when asked for general UI feedback, but specific guidance produces higher quality and more diverse suggestions.",
                                        "sentences": [
                                            {
                                                "id": "sec-17_p1_s1",
                                                "text": "Finally, we investigate how GPT-4 responds without specific guidance when prompted for general UI feedback.",
                                                "context": "Section 3.5.3: General UI Feedback",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-17_p1_s2",
                                                "text": "We removed all mentions of \u201cguidelines\u201d in the prompt and replaced \u201cviolations\u201d with \u201cfeedback.\u201d Quantitatively, the performance for this condition was worse.",
                                                "context": "Section 3.5.3: General UI Feedback",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-17_p1_s3",
                                                "text": "Qualitatively, GPT-4 still carried out heuristic evaluation to an extent, as the issues were grounded in existing design conventions, but in a less rigorous and organized manner.",
                                                "context": "Section 3.5.3: General UI Feedback",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-17_p1_s4",
                                                "text": "Compared to the complete prompt, the feedback was less diverse, and the LLM often focused on only one type of issue (e.g., misalignment) when there were other types of violations.",
                                                "context": "Section 3.5.3: General UI Feedback",
                                                "associated_visual": "table1"
                                            },
                                            {
                                                "id": "sec-17_p1_s5",
                                                "text": "We conclude that GPT-4 can produce plausible output when asked for general UI feedback, but specific guidance produces higher quality and more diverse suggestions.",
                                                "context": "Section 3.5.3: General UI Feedback",
                                                "associated_visual": "table1"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            }
                        ]
                    },
                    {
                        "id": "sec-18",
                        "section_number": "3.6",
                        "title": "Comparison with other LLMs",
                        "paragraphs": [
                            {
                                "full_text": "We explored the potential of other state-of-the-art LLMs in carrying out this task: Claude 2, GPT-3.5-turbo-16k, and PaLM 2. Llama 2 was considered but excluded because its 4k context window size is insufficient for the task. Similar to the prompt analysis, we compute the total number of violations found and the number of helpful violations.",
                                "sentences": [
                                    {
                                        "id": "sec-18_p1_s1",
                                        "text": "We explored the potential of other state-of-the-art LLMs in carrying out this task: Claude 2, GPT-3.5-turbo-16k, and PaLM 2.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p1_s2",
                                        "text": "Llama 2 was considered but excluded because its 4k context window size is insufficient for the task.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p1_s3",
                                        "text": "Similar to the prompt analysis, we compute the total number of violations found and the number of helpful violations.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    }
                                ]
                            },
                            {
                                "full_text": "We found that Claude 2, GPT-3.5-turbo-16k, and PaLM 2 all had considerably worse performance than GPT-4, as shown in Table 3.5 (bottom). Claude 2 and PaLM 2 found very few violations; Claude 2 only found violations in 4 UIs, and PaLM 2 only identified one violation per UI, even after adjusting the prompt to indicate more than one violation per UI. In fact, all 12 UIs have multiple violations, as later confirmed in a heuristic evaluation by human experts. The few violations found by these two LLMs were mostly unhelpful, such as suggesting the dollar sign needs a text label. GPT-3.5-turbo-16k had the opposite behavior, finding nearly 4 times as many violations as GPT-4. However, most of the time, it indiscriminately applied the same guideline to every element of the appropriate type, regardless if there is an issue (e.g., stating the font is difficult to read for every text element). This behavior also meant that most of its helpful violations were found by chance, despite finding fewer helpful violations than GPT-4. Finally, GPT-3.5-turbo-16k and PaLM 2 had difficulty following the prompt's instructions, often formatting the output incorrectly (with a separate rephrasing call) or making the mistakes they were told to avoid, such as returning violations regarding the mobile status bar.",
                                "sentences": [
                                    {
                                        "id": "sec-18_p2_s1",
                                        "text": "We found that Claude 2, GPT-3.5-turbo-16k, and PaLM 2 all had considerably worse performance than GPT-4, as shown in Table 3.5 (bottom).",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s2",
                                        "text": "Claude 2 and PaLM 2 found very few violations; Claude 2 only found violations in 4 UIs, and PaLM 2 only identified one violation per UI, even after adjusting the prompt to indicate more than one violation per UI.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s3",
                                        "text": "In fact, all 12 UIs have multiple violations, as later confirmed in a heuristic evaluation by human experts.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s4",
                                        "text": "The few violations found by these two LLMs were mostly unhelpful, such as suggesting the dollar sign needs a text label.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s5",
                                        "text": "GPT-3.5-turbo-16k had the opposite behavior, finding nearly 4 times as many violations as GPT-4.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s6",
                                        "text": "However, most of the time, it indiscriminately applied the same guideline to every element of the appropriate type, regardless if there is an issue (e.g., stating the font is difficult to read for every text element).",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s7",
                                        "text": "This behavior also meant that most of its helpful violations were found by chance, despite finding fewer helpful violations than GPT-4.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p2_s8",
                                        "text": "Finally, GPT-3.5-turbo-16k and PaLM 2 had difficulty following the prompt's instructions, often formatting the output incorrectly (with a separate rephrasing call) or making the mistakes they were told to avoid, such as returning violations regarding the mobile status bar.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    }
                                ]
                            },
                            {
                                "full_text": "These models are all smaller than GPT-4, with billions of parameters, compared to GPT-4\u2019s 1.7 trillion [42]. These models have also been shown to have worse reasoning skills [3]. These factors likely contributed to their poor performance in this task. Since GPT-4 has the best performance by far, we solely focus on GPT-4 for the remaining three studies on the plugin.",
                                "sentences": [
                                    {
                                        "id": "sec-18_p3_s1",
                                        "text": "These models are all smaller than GPT-4, with billions of parameters, compared to GPT-4\u2019s 1.7 trillion [42].",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p3_s2",
                                        "text": "These models have also been shown to have worse reasoning skills [3].",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p3_s3",
                                        "text": "These factors likely contributed to their poor performance in this task.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    },
                                    {
                                        "id": "sec-18_p3_s4",
                                        "text": "Since GPT-4 has the best performance by far, we solely focus on GPT-4 for the remaining three studies on the plugin.",
                                        "context": "Section 3.6: Comparison with other LLMs",
                                        "associated_visual": "table1"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    }
                ]
            },
            {
                "id": "sec-19",
                "section_number": "4",
                "title": "STUDY METHOD",
                "paragraphs": [
                    {
                        "full_text": "To explore the potential of GPT-4 in automating heuristic evaluation, we carried out three studies (see Figure 5). In the Performance study, three designers rated the accuracy and helpfulness of GPT-4\u2019s generated suggestions for 51 diverse UI mockups to establish performance metrics across a variety of designs. Next, we conducted a heuristic evaluation study with 12 design experts, who each manually identified guideline violations in 6 UIs. Afterwards, they compared their identified violations with those found by GPT-4 in an interview. Finally, in the Iterative Usage study, another group of 12 designers iteratively refined three UIs each with the tool and discussed how the tool might fit into existing workflows in an interview. We obtained UIs from the Figma Community, where designers share their mockups publicly. To attain a diverse set of UIs, we searched for UIs from various app categories, such as finance and e-commerce. We selected UIs that have room for improvement (based on our guidelines) and have JSON representations that could fit into GPT-4\u2019s context window. We only used mobile UIs because web UIs were usually too large. For each UI, we ensured that the grouping structure in the Layers panel matched the visual grouping structure in the UI screenshot. We also used our tool to automatically generate semantically informative names for unnamed groups in the Layers panel.",
                        "sentences": [
                            {
                                "id": "sec-19_p1_s1",
                                "text": "To explore the potential of GPT-4 in automating heuristic evaluation, we carried out three studies (see Figure 5).",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s2",
                                "text": "In the Performance study, three designers rated the accuracy and helpfulness of GPT-4\u2019s generated suggestions for 51 diverse UI mockups to establish performance metrics across a variety of designs.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s3",
                                "text": "Next, we conducted a heuristic evaluation study with 12 design experts, who each manually identified guideline violations in 6 UIs.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s4",
                                "text": "Afterwards, they compared their identified violations with those found by GPT-4 in an interview.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s5",
                                "text": "Finally, in the Iterative Usage study, another group of 12 designers iteratively refined three UIs each with the tool and discussed how the tool might fit into existing workflows in an interview.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s6",
                                "text": "We obtained UIs from the Figma Community, where designers share their mockups publicly.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s7",
                                "text": "To attain a diverse set of UIs, we searched for UIs from various app categories, such as finance and e-commerce.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s8",
                                "text": "We selected UIs that have room for improvement (based on our guidelines) and have JSON representations that could fit into GPT-4\u2019s context window.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s9",
                                "text": "We only used mobile UIs because web UIs were usually too large.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s10",
                                "text": "For each UI, we ensured that the grouping structure in the Layers panel matched the visual grouping structure in the UI screenshot.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            },
                            {
                                "id": "sec-19_p1_s11",
                                "text": "We also used our tool to automatically generate semantically informative names for unnamed groups in the Layers panel.",
                                "context": "Section 4: STUDY METHOD",
                                "associated_visual": "fig5"
                            }
                        ]
                    }
                ],
                "lists": [],
                "subsections": [
                    {
                        "id": "sec-20",
                        "section_number": "4.1",
                        "title": "Performance Study",
                        "paragraphs": [
                            {
                                "full_text": "We recruited three designers for the Performance study through advertising at an academic institution. Each participant had 3-4 years of design experience, and their areas of expertise include mobile, web, product, and UX design. This background information was collected during a brief instructional meeting conducted prior to participants starting this task. We precomputed the guideline violations for all 51 UIs to ensure that all participants saw the same suggestions, allowing us to calculate inter-rater agreement. The 51 UIs were split into three groups of 17, and each group was evaluated using one set of guidelines. Each participant saw the same set of 51 UIs and were given a week to rate the suggestions. Participants spent an average of 6.8 hours total on this task.",
                                "sentences": [
                                    {
                                        "id": "sec-20_p1_s1",
                                        "text": "We recruited three designers for the Performance study through advertising at an academic institution.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p1_s2",
                                        "text": "Each participant had 3-4 years of design experience, and their areas of expertise include mobile, web, product, and UX design.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p1_s3",
                                        "text": "This background information was collected during a brief instructional meeting conducted prior to participants starting this task.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p1_s4",
                                        "text": "We precomputed the guideline violations for all 51 UIs to ensure that all participants saw the same suggestions, allowing us to calculate inter-rater agreement.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p1_s5",
                                        "text": "The 51 UIs were split into three groups of 17, and each group was evaluated using one set of guidelines.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p1_s6",
                                        "text": "Each participant saw the same set of 51 UIs and were given a week to rate the suggestions.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p1_s7",
                                        "text": "Participants spent an average of 6.8 hours total on this task.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    }
                                ]
                            },
                            {
                                "full_text": "For each suggestion, participants were asked to select a rating for accuracy on a scale of 1 to 3 (\u201c1 - not accurate\u201d, \u201c2 - partially accurate\u201d, \u201c3 - accurate\u201d) and then provide a brief, one-sentence explanation for their rating. Participants were also asked to rate the suggestion's helpfulness on a scale of 1 to 5, with 1 being \u201cnot at all helpful\u201d and 5 being \u201cvery helpful\u201d, and also provide a brief explanation. We stored all GPT-4 suggestions, along with the corresponding anonymized rating data, explanations, and UI JSONs from this study, and have made this dataset available in the Supplementary Materials.",
                                "sentences": [
                                    {
                                        "id": "sec-20_p2_s1",
                                        "text": "For each suggestion, participants were asked to select a rating for accuracy on a scale of 1 to 3 (\u201c1 - not accurate\u201d, \u201c2 - partially accurate\u201d, \u201c3 - accurate\u201d) and then provide a brief, one-sentence explanation for their rating.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p2_s2",
                                        "text": "Participants were also asked to rate the suggestion's helpfulness on a scale of 1 to 5, with 1 being \u201cnot at all helpful\u201d and 5 being \u201cvery helpful\u201d, and also provide a brief explanation.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-20_p2_s3",
                                        "text": "We stored all GPT-4 suggestions, along with the corresponding anonymized rating data, explanations, and UI JSONs from this study, and have made this dataset available in the Supplementary Materials.",
                                        "context": "Section 4.1: Performance Study",
                                        "associated_visual": "fig5"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-21",
                        "section_number": "4.2",
                        "title": "Manual Heuristic Evaluation Study with Human Experts",
                        "paragraphs": [
                            {
                                "full_text": "We recruited 12 participants through advertising at a large technology company and an academic institution. Two participants had less than 3 years of design experience, six had 3-5 years, two had 6-10 years, and one had 15 years. Their areas of expertise include mobile, web, product, UX, cross device, and UX and UI research. The study was conducted remotely during a 90-minute session, where participants looked for guideline violations in 6 UIs in a Figma file. Each UI was assigned one of the sets of guidelines for evaluation.",
                                "sentences": [
                                    {
                                        "id": "sec-21_p1_s1",
                                        "text": "We recruited 12 participants through advertising at a large technology company and an academic institution.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p1_s2",
                                        "text": "Two participants had less than 3 years of design experience, six had 3-5 years, two had 6-10 years, and one had 15 years.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p1_s3",
                                        "text": "Their areas of expertise include mobile, web, product, UX, cross device, and UX and UI research.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p1_s4",
                                        "text": "The study was conducted remotely during a 90-minute session, where participants looked for guideline violations in 6 UIs in a Figma file.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p1_s5",
                                        "text": "Each UI was assigned one of the sets of guidelines for evaluation.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    }
                                ]
                            },
                            {
                                "full_text": "The first 75 minutes consisted of the heuristic evaluation. Participants were instructed to provide the name of the guideline violated, an explanation of the violation following [48], and a usability severity rating for each violation found. There were a total of 12 UIs used for this study, and each UI was evaluated by 6 participants. The remaining 15 minutes were allocated for a semi-structured interview, where we demoed the plugin and generated feedback for the same 6 UIs the participant evaluated. We then asked the participants to compare the LLM's violations with their own.",
                                "sentences": [
                                    {
                                        "id": "sec-21_p2_s1",
                                        "text": "The first 75 minutes consisted of the heuristic evaluation.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p2_s2",
                                        "text": "Participants were instructed to provide the name of the guideline violated, an explanation of the violation following [48], and a usability severity rating for each violation found.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p2_s3",
                                        "text": "There were a total of 12 UIs used for this study, and each UI was evaluated by 6 participants.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p2_s4",
                                        "text": "The remaining 15 minutes were allocated for a semi-structured interview, where we demoed the plugin and generated feedback for the same 6 UIs the participant evaluated.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-21_p2_s5",
                                        "text": "We then asked the participants to compare the LLM's violations with their own.",
                                        "context": "Section 4.2: Manual Heuristic Evaluation Study with Human Experts",
                                        "associated_visual": "fig5"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-22",
                        "section_number": "4.3",
                        "title": "Iterative Usage Study",
                        "paragraphs": [
                            {
                                "full_text": "We recruited another group of 12 participants through advertising at an academic institution and a large technology company. One participant had less than 3 years of design experience, five had 3-5 years, three had 6-10 years, two had 11-15 years, and one had over 32 years. Their areas of expertise include mobile, web, product, UX, mixed reality design, and UX and HCI research. The study was conducted either in-person or remotely during a 90-minute session. Participants were given three UIs in a Figma file, each with their corresponding heuristics assigned for the evaluation. Participants worked through one UI at a time. They first rated the accuracy and helpfulness of GPT-4\u2019s suggestions, following the scales used in the Performance Study. However, participants in the Usage study were asked to follow helpful suggestions to edit the mockup, though they could skip revisions that require too much work, like restructuring the entire layout. After participants finished revising the UI, they would rerun the plugin to generate a new set of suggestions for the revised mockup and then re-rate the new suggestions. For UIs 1 and 3, participants did one round of edits and two rounds of ratings. For UI 2, participants did two rounds of edits and three rounds of ratings, which is meant to assess the LLM's iterative performance. This study used the same set of 12 UIs as the manual heuristic evaluation study (with the same guideline assignments for each UI's evaluation), and each UI was seen by three participants. To assess rater agreement, we again precomputed the first round suggestions for each UI. After participants finished all three tasks, we concluded with a semi-structured interview, focusing on overall impressions, potential drawbacks and dangers, potential for iterative use, and fit with their design workflow.",
                                "sentences": [
                                    {
                                        "id": "sec-22_p1_s1",
                                        "text": "We recruited another group of 12 participants through advertising at an academic institution and a large technology company.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s2",
                                        "text": "One participant had less than 3 years of design experience, five had 3-5 years, three had 6-10 years, two had 11-15 years, and one had over 32 years.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s3",
                                        "text": "Their areas of expertise include mobile, web, product, UX, mixed reality design, and UX and HCI research.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s4",
                                        "text": "The study was conducted either in-person or remotely during a 90-minute session.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s5",
                                        "text": "Participants were given three UIs in a Figma file, each with their corresponding heuristics assigned for the evaluation.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s6",
                                        "text": "Participants worked through one UI at a time.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s7",
                                        "text": "They first rated the accuracy and helpfulness of GPT-4\u2019s suggestions, following the scales used in the Performance Study.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s8",
                                        "text": "However, participants in the Usage study were asked to follow helpful suggestions to edit the mockup, though they could skip revisions that require too much work, like restructuring the entire layout.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s9",
                                        "text": "After participants finished revising the UI, they would rerun the plugin to generate a new set of suggestions for the revised mockup and then re-rate the new suggestions.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s10",
                                        "text": "For UIs 1 and 3, participants did one round of edits and two rounds of ratings.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s11",
                                        "text": "For UI 2, participants did two rounds of edits and three rounds of ratings, which is meant to assess the LLM's iterative performance.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s12",
                                        "text": "This study used the same set of 12 UIs as the manual heuristic evaluation study (with the same guideline assignments for each UI's evaluation), and each UI was seen by three participants.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s13",
                                        "text": "To assess rater agreement, we again precomputed the first round suggestions for each UI.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    },
                                    {
                                        "id": "sec-22_p1_s14",
                                        "text": "After participants finished all three tasks, we concluded with a semi-structured interview, focusing on overall impressions, potential drawbacks and dangers, potential for iterative use, and fit with their design workflow.",
                                        "context": "Section 4.3: Iterative Usage Study",
                                        "associated_visual": "fig5"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    }
                ]
            },
            {
                "id": "sec-23",
                "section_number": "5",
                "title": "RESULTS",
                "paragraphs": [],
                "lists": [],
                "subsections": [
                    {
                        "id": "sec-24",
                        "section_number": "5.1",
                        "title": "Quantitative Results: Performance Study",
                        "paragraphs": [
                            {
                                "full_text": "More GPT-4 generated suggestions were rated as accurate and helpful than not. Across all generated suggestions, 52 percent were rated Accurate, 19 percent Partially Accurate, and 29 percent Not Accurate; 49 percent were considered helpful or very helpful, 15 percent moderately helpful, and 36 percent slightly or not at all helpful. We show histograms of ratings given to all suggestions for each set of guidelines in Figure 6, along with averages. In line with the aggregate statistics, GPT-4 is more accurate and helpful than not for each set of guidelines. Furthermore, this difference is largest for CrowdCrit's visual guidelines and smallest for the Semantic Grouping guidelines. Regarding the average rating for all suggestions, CrowdCrit outperformed the other guidelines for accuracy and helpfulness, with a greater outperformance in helpfulness. Semantic Grouping had the worst performance for accuracy, and Nielsen Norman performed the worst for helpfulness. Later, we show concrete examples of GPT-4 generated feedback in Figure 9 that includes accurate and helpful suggestions, as well as inaccurate and unhelpful ones.",
                                "sentences": [
                                    {
                                        "id": "sec-24_p1_s1",
                                        "text": "More GPT-4 generated suggestions were rated as accurate and helpful than not.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s2",
                                        "text": "Across all generated suggestions, 52 percent were rated Accurate, 19 percent Partially Accurate, and 29 percent Not Accurate; 49 percent were considered helpful or very helpful, 15 percent moderately helpful, and 36 percent slightly or not at all helpful.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s3",
                                        "text": "We show histograms of ratings given to all suggestions for each set of guidelines in Figure 6, along with averages.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s4",
                                        "text": "In line with the aggregate statistics, GPT-4 is more accurate and helpful than not for each set of guidelines.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s5",
                                        "text": "Furthermore, this difference is largest for CrowdCrit's visual guidelines and smallest for the Semantic Grouping guidelines.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s6",
                                        "text": "Regarding the average rating for all suggestions, CrowdCrit outperformed the other guidelines for accuracy and helpfulness, with a greater outperformance in helpfulness.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s7",
                                        "text": "Semantic Grouping had the worst performance for accuracy, and Nielsen Norman performed the worst for helpfulness.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    },
                                    {
                                        "id": "sec-24_p1_s8",
                                        "text": "Later, we show concrete examples of GPT-4 generated feedback in Figure 9 that includes accurate and helpful suggestions, as well as inaccurate and unhelpful ones.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig6"
                                    }
                                ]
                            },
                            {
                                "full_text": "We also grouped the ratings by individual guideline and visualized them in horizontal bar charts shown in Figure 7. This reveals finer-grained types of heuristics on which GPT-4 performed better than others. The accuracy was highest for the \u201cRecognition rather than Recall\u201d, \u201cMatch Between System and Real World\u201d, and \u201cConsistency and Standards\u201d usability heuristics (highlighted in green in Figure 7), and participants generally found \u201cConsistency and Standards\u201d violations helpful. \u201cConsistency and Standards\u201d mostly caught inconsistencies in the visual layout of the UI, like misalignment and inconsistency in size. In contrast, the \u201cAesthetic and Minimalist Design\u201d guideline was generally inaccurate and hence unhelpful, as shown in red in Figure 7. Finally, the \u201cEmphasis\u201d principle (shown in orange) from CrowdCrit was bimodal \u2013 a fairly even distribution of \u201caccurate\u201d and \u201cinaccurate\u201d, as well as \u201cvery helpful\u201d and \u201cunhelpful ratings\u201d. The \u201cEmphasis\u201d principle mostly identified issues related to the visual hierarchy of the UI.",
                                "sentences": [
                                    {
                                        "id": "sec-24_p2_s1",
                                        "text": "We also grouped the ratings by individual guideline and visualized them in horizontal bar charts shown in Figure 7.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p2_s2",
                                        "text": "This reveals finer-grained types of heuristics on which GPT-4 performed better than others.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p2_s3",
                                        "text": "The accuracy was highest for the \u201cRecognition rather than Recall\u201d, \u201cMatch Between System and Real World\u201d, and \u201cConsistency and Standards\u201d usability heuristics (highlighted in green in Figure 7), and participants generally found \u201cConsistency and Standards\u201d violations helpful. \u201cConsistency and Standards\u201d mostly caught inconsistencies in the visual layout of the UI, like misalignment and inconsistency in size.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p2_s4",
                                        "text": "In contrast, the \u201cAesthetic and Minimalist Design\u201d guideline was generally inaccurate and hence unhelpful, as shown in red in Figure 7.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p2_s5",
                                        "text": "Finally, the \u201cEmphasis\u201d principle (shown in orange) from CrowdCrit was bimodal \u2013 a fairly even distribution of \u201caccurate\u201d and \u201cinaccurate\u201d, as well as \u201cvery helpful\u201d and \u201cunhelpful ratings\u201d.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p2_s6",
                                        "text": "The \u201cEmphasis\u201d principle mostly identified issues related to the visual hierarchy of the UI.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    }
                                ]
                            },
                            {
                                "full_text": "The subjective nature of heuristic evaluation was already highlighted by Nielsen [40]. To characterize subjectivity in our study, we computed inter-rater reliability using Fleiss\u2019 Kappa [13]. Accuracy ratings had an agreement score of 0.112 and helpfulness ratings had a score of 0.100, which suggests only slight agreement. In addition to the subjective nature of this task, particular choices in the phrasing of the suggestions could have also lowered agreement scores. For example, the suggestion \u201cThe icons in this group... could be more user-friendly with the addition of text labels\u201d calls for a subjective opinion on whether or not text labels are needed, and raters might reasonably disagree.",
                                "sentences": [
                                    {
                                        "id": "sec-24_p3_s1",
                                        "text": "The subjective nature of heuristic evaluation was already highlighted by Nielsen [40].",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p3_s2",
                                        "text": "To characterize subjectivity in our study, we computed inter-rater reliability using Fleiss\u2019 Kappa [13].",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p3_s3",
                                        "text": "Accuracy ratings had an agreement score of 0.112 and helpfulness ratings had a score of 0.100, which suggests only slight agreement.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p3_s4",
                                        "text": "In addition to the subjective nature of this task, particular choices in the phrasing of the suggestions could have also lowered agreement scores.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    },
                                    {
                                        "id": "sec-24_p3_s5",
                                        "text": "For example, the suggestion \u201cThe icons in this group... could be more user-friendly with the addition of text labels\u201d calls for a subjective opinion on whether or not text labels are needed, and raters might reasonably disagree.",
                                        "context": "Section 5.1: Quantitative Results: Performance Study",
                                        "associated_visual": "fig7"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-25",
                        "section_number": "5.2",
                        "title": "Quantitative Results: Comparison with Human Evaluators",
                        "paragraphs": [
                            {
                                "full_text": "We compiled all violations identified by the 12 experts from the manual heuristic evaluation study. In total, the experts found 72 distinct guideline violations in the 12 UIs. However, participants sometimes combined multiple violations for a single group or element (e.g., \u201cThe spacing between each icon is inconsistent, and the entire bottom menu is not centered.\u201d). After splitting these combined violations into separate issues, the count increased to 91 distinct violations. GPT-4 found 38 helpful violations for the same set of 12 UIs (determined from the Usage study). Nine of these violations were missed by human experts, so in total, the experts and GPT-4 found 100 distinct violations. In summary, 9 violations were found by GPT-4 only, 29 were found by both GPT-4 and human experts, and 62 were found by human experts only.",
                                "sentences": [
                                    {
                                        "id": "sec-25_p1_s1",
                                        "text": "We compiled all violations identified by the 12 experts from the manual heuristic evaluation study.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p1_s2",
                                        "text": "In total, the experts found 72 distinct guideline violations in the 12 UIs.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p1_s3",
                                        "text": "However, participants sometimes combined multiple violations for a single group or element (e.g., \u201cThe spacing between each icon is inconsistent, and the entire bottom menu is not centered.\u201d).",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p1_s4",
                                        "text": "After splitting these combined violations into separate issues, the count increased to 91 distinct violations.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p1_s5",
                                        "text": "GPT-4 found 38 helpful violations for the same set of 12 UIs (determined from the Usage study).",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p1_s6",
                                        "text": "Nine of these violations were missed by human experts, so in total, the experts and GPT-4 found 100 distinct violations.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p1_s7",
                                        "text": "In summary, 9 violations were found by GPT-4 only, 29 were found by both GPT-4 and human experts, and 62 were found by human experts only.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    }
                                ]
                            },
                            {
                                "full_text": "We built a ground truth dataset consisting of these 100 violations and computed precision, recall, and F1 performance metrics for GPT-4, which can be found in Table 2. We also compute the average performance for an individual human evaluator, by averaging these metrics across all participants, which can also be found in Table 2. On average, a human evaluator had higher precision than GPT-4, which means the guideline violations they found were more likely to be helpful. The average human precision is less than 1, because study participants sometimes found issues irrelevant to the set of heuristics used (e.g., recorded visual grouping issues when using the Semantic Grouping guidelines). GPT-4 scored slightly higher in recall and slightly lower in the F1 score than the average human evaluator, though both these differences are much smaller than the difference in precision.",
                                "sentences": [
                                    {
                                        "id": "sec-25_p2_s1",
                                        "text": "We built a ground truth dataset consisting of these 100 violations and computed precision, recall, and F1 performance metrics for GPT-4, which can be found in Table 2.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p2_s2",
                                        "text": "We also compute the average performance for an individual human evaluator, by averaging these metrics across all participants, which can also be found in Table 2.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p2_s3",
                                        "text": "On average, a human evaluator had higher precision than GPT-4, which means the guideline violations they found were more likely to be helpful.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p2_s4",
                                        "text": "The average human precision is less than 1, because study participants sometimes found issues irrelevant to the set of heuristics used (e.g., recorded visual grouping issues when using the Semantic Grouping guidelines).",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    },
                                    {
                                        "id": "sec-25_p2_s5",
                                        "text": "GPT-4 scored slightly higher in recall and slightly lower in the F1 score than the average human evaluator, though both these differences are much smaller than the difference in precision.",
                                        "context": "Section 5.2: Quantitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "table2"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-26",
                        "section_number": "5.3",
                        "title": "Quantitative Results: Iterative Usage Study",
                        "paragraphs": [
                            {
                                "full_text": "We collected accuracy and helpfulness scores in the Usage study. Statistics on ratings for suggestions on initial UIs, before participants made any changes, closely match that of the Performance Study: 52 percent were rated Accurate, 28 percent Partially Accurate, and 20 percent Not Accurate; 47 percent were considered helpful or very helpful, 19 percent moderately helpful, and 34 percent slightly or not at all helpful (see Figure 8).",
                                "sentences": [
                                    {
                                        "id": "sec-26_p1_s1",
                                        "text": "We collected accuracy and helpfulness scores in the Usage study.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p1_s2",
                                        "text": "Statistics on ratings for suggestions on initial UIs, before participants made any changes, closely match that of the Performance Study: 52 percent were rated Accurate, 28 percent Partially Accurate, and 20 percent Not Accurate; 47 percent were considered helpful or very helpful, 19 percent moderately helpful, and 34 percent slightly or not at all helpful (see Figure 8).",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    }
                                ]
                            },
                            {
                                "full_text": "However, score distributions were lower for later rounds, after participants edited the UIs. Namely, 39 percent were rated Accurate, 26 percent Partially Accurate, and 35 percent Not Accurate; 33 percent were considered helpful or very helpful, 16 percent moderately helpful, and 51 percent slightly or not at all helpful. This discrepancy suggests that participants\u2019 opinions of suggestions changed during the iterative re-design process. To investigate this, we examined the ratings given per round of iteration in the Usage Study. Since GPT-4\u2019s suggestions vary in accuracy and helpfulness, we used a horizontal bar chart to show the distributions of all ratings given to suggestions in each round. Figure 8 shows a general trend of decreasing performance per round for both accuracy and helpfulness. This trend also generally holds for both metrics when broken down by guidelines and participants (available in the Supplementary Materials).",
                                "sentences": [
                                    {
                                        "id": "sec-26_p2_s1",
                                        "text": "However, score distributions were lower for later rounds, after participants edited the UIs.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p2_s2",
                                        "text": "Namely, 39 percent were rated Accurate, 26 percent Partially Accurate, and 35 percent Not Accurate; 33 percent were considered helpful or very helpful, 16 percent moderately helpful, and 51 percent slightly or not at all helpful.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p2_s3",
                                        "text": "This discrepancy suggests that participants\u2019 opinions of suggestions changed during the iterative re-design process.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p2_s4",
                                        "text": "To investigate this, we examined the ratings given per round of iteration in the Usage Study.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p2_s5",
                                        "text": "Since GPT-4\u2019s suggestions vary in accuracy and helpfulness, we used a horizontal bar chart to show the distributions of all ratings given to suggestions in each round.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p2_s6",
                                        "text": "Figure 8 shows a general trend of decreasing performance per round for both accuracy and helpfulness.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    },
                                    {
                                        "id": "sec-26_p2_s7",
                                        "text": "This trend also generally holds for both metrics when broken down by guidelines and participants (available in the Supplementary Materials).",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    }
                                ]
                            },
                            {
                                "full_text": "The average inter-rater reliability score, based on the first round of suggestions, is 0.155 for accuracy and 0.085 for helpfulness, which again indicates subjectivity in the experts\u2019 opinions towards the suggestions.",
                                "sentences": [
                                    {
                                        "id": "sec-26_p3_s1",
                                        "text": "The average inter-rater reliability score, based on the first round of suggestions, is 0.155 for accuracy and 0.085 for helpfulness, which again indicates subjectivity in the experts\u2019 opinions towards the suggestions.",
                                        "context": "Section 5.3: Quantitative Results: Iterative Usage Study",
                                        "associated_visual": "fig8"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-27",
                        "section_number": "5.4",
                        "title": "Qualitative Results: GPT-4 Strengths and Weakness",
                        "paragraphs": [
                            {
                                "full_text": "We analyzed GPT-4\u2019s suggestions, corresponding expert ratings, explanations, and interview responses from the Usage study. Through grounded theory coding [16] of the qualitative data and subsequent thematic analysis [4], we identified the following emerging themes on GPT-4\u2019s strengths and weaknesses. Figure 9 contains examples of high and low-rated LLM suggestions to illustrate some of these themes.",
                                "sentences": [
                                    {
                                        "id": "sec-27_p1_s1",
                                        "text": "We analyzed GPT-4\u2019s suggestions, corresponding expert ratings, explanations, and interview responses from the Usage study.",
                                        "context": "Section 5.4: Qualitative Results: GPT-4 Strengths and Weakness",
                                        "associated_visual": "fig9"
                                    },
                                    {
                                        "id": "sec-27_p1_s2",
                                        "text": "Through grounded theory coding [16] of the qualitative data and subsequent thematic analysis [4], we identified the following emerging themes on GPT-4\u2019s strengths and weaknesses.",
                                        "context": "Section 5.4: Qualitative Results: GPT-4 Strengths and Weakness",
                                        "associated_visual": "fig9"
                                    },
                                    {
                                        "id": "sec-27_p1_s3",
                                        "text": "Figure 9 contains examples of high and low-rated LLM suggestions to illustrate some of these themes.",
                                        "context": "Section 5.4: Qualitative Results: GPT-4 Strengths and Weakness",
                                        "associated_visual": "fig9"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": [
                            {
                                "id": "sec-28",
                                "section_number": "5.4.1",
                                "title": "Strength 1: Identification of Subtle Issues (12/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "All participants found GPT-4\u2019s ability to identify subtle, easy-to-miss issues helpful. This includes problems like misalignment, uneven spacing, poor color contrast, redundant elements, and uncommon icons without text labels. UI B in Figure 9 contains an example of a misalignment caught by GPT-4 that all three participants found very helpful. Seven participants mentioned this theme as a strength during the interview. For instance, P5 stated that the tool is \u201cuseful for pointing out things that are not obvious to the naked eye, like minor visual details\u201d, and P4 said that they \u201cliked the UI at first, but then the LLM found small issues with labels, etc.\u201d",
                                        "sentences": [
                                            {
                                                "id": "sec-28_p1_s1",
                                                "text": "All participants found GPT-4\u2019s ability to identify subtle, easy-to-miss issues helpful.",
                                                "context": "Section 5.4.1: Strength 1: Identification of Subtle Issues (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-28_p1_s2",
                                                "text": "This includes problems like misalignment, uneven spacing, poor color contrast, redundant elements, and uncommon icons without text labels.",
                                                "context": "Section 5.4.1: Strength 1: Identification of Subtle Issues (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-28_p1_s3",
                                                "text": "UI B in Figure 9 contains an example of a misalignment caught by GPT-4 that all three participants found very helpful.",
                                                "context": "Section 5.4.1: Strength 1: Identification of Subtle Issues (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-28_p1_s4",
                                                "text": "Seven participants mentioned this theme as a strength during the interview.",
                                                "context": "Section 5.4.1: Strength 1: Identification of Subtle Issues (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-28_p1_s5",
                                                "text": "For instance, P5 stated that the tool is \u201cuseful for pointing out things that are not obvious to the naked eye, like minor visual details\u201d, and P4 said that they \u201cliked the UI at first, but then the LLM found small issues with labels, etc.\u201d",
                                                "context": "Section 5.4.1: Strength 1: Identification of Subtle Issues (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-29",
                                "section_number": "5.4.2",
                                "title": "Strength 2: Fixing Text-related Issues in the UI (12/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "GPT-4 was also effective in identifying text-related issues, like incorrect grammar, unclear text labels, and text that is not user-friendly (e.g., uses too much jargon). In addition, GPT-4 would usually include the correct text to use in its feedback. For instance, one UI had a grammatically incorrect header: \u201cwhat kind of pet your Looking?\u201d and GPT-4 suggested revising it to \u201cWhat kind of pet are you looking for?\u201d P1 rated this suggestion as very helpful and explained that \u201cit gave me the right content to copy and use in the design.\u201d",
                                        "sentences": [
                                            {
                                                "id": "sec-29_p1_s1",
                                                "text": "GPT-4 was also effective in identifying text-related issues, like incorrect grammar, unclear text labels, and text that is not user-friendly (e.g., uses too much jargon).",
                                                "context": "Section 5.4.2: Strength 2: Fixing Text-related Issues in the UI (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-29_p1_s2",
                                                "text": "In addition, GPT-4 would usually include the correct text to use in its feedback.",
                                                "context": "Section 5.4.2: Strength 2: Fixing Text-related Issues in the UI (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-29_p1_s3",
                                                "text": "For instance, one UI had a grammatically incorrect header: \u201cwhat kind of pet your Looking?\u201d and GPT-4 suggested revising it to \u201cWhat kind of pet are you looking for?\u201d P1 rated this suggestion as very helpful and explained that \u201cit gave me the right content to copy and use in the design.\u201d",
                                                "context": "Section 5.4.2: Strength 2: Fixing Text-related Issues in the UI (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-30",
                                "section_number": "5.4.3",
                                "title": "Strength 3: Reasoning with UI Semantics (8/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "GPT-4 was skilled at reasoning with UI semantics. This includes identifying large groups of items that could be subgrouped into smaller, more semantically-related groups. This is illustrated in UI A of Figure 9, where the LLM suggested subgrouping the menu items into more related categories. P2 commented \u201cYes, I think it would be helpful to separate out the sections into categories, as some relate to the purpose of the application (traveling), and other parts are related to basic maintenance of the account (e.g., settings)\u201d. Other semantics-related issues that GPT-4 identified include finding groups or subgroups with elements are not clearly related, groups or visualizations where the purpose is unclear and requires a text label to explain them, and confusing UIs that require documentation.",
                                        "sentences": [
                                            {
                                                "id": "sec-30_p1_s1",
                                                "text": "GPT-4 was skilled at reasoning with UI semantics.",
                                                "context": "Section 5.4.3: Strength 3: Reasoning with UI Semantics (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-30_p1_s2",
                                                "text": "This includes identifying large groups of items that could be subgrouped into smaller, more semantically-related groups.",
                                                "context": "Section 5.4.3: Strength 3: Reasoning with UI Semantics (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-30_p1_s3",
                                                "text": "This is illustrated in UI A of Figure 9, where the LLM suggested subgrouping the menu items into more related categories.",
                                                "context": "Section 5.4.3: Strength 3: Reasoning with UI Semantics (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-30_p1_s4",
                                                "text": "P2 commented \u201cYes, I think it would be helpful to separate out the sections into categories, as some relate to the purpose of the application (traveling), and other parts are related to basic maintenance of the account (e.g., settings)\u201d.",
                                                "context": "Section 5.4.3: Strength 3: Reasoning with UI Semantics (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-30_p1_s5",
                                                "text": "Other semantics-related issues that GPT-4 identified include finding groups or subgroups with elements are not clearly related, groups or visualizations where the purpose is unclear and requires a text label to explain them, and confusing UIs that require documentation.",
                                                "context": "Section 5.4.3: Strength 3: Reasoning with UI Semantics (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-31",
                                "section_number": "5.4.4",
                                "title": "Other Strengths",
                                "paragraphs": [
                                    {
                                        "full_text": "For two participants, GPT-4 made clever, high-level suggestions. For instance, GPT-4 recommended that a product quantity value be changed to an editable field, so users will not have to rely on the \u2018+\u2019 or \u2018-\u2019 buttons to adjust the quantity, and cited Nielsen's \u201cFlexibility and Efficiency of Use\u201d heuristic. In addition, GPT-4 suggested that a list of reviews for a cafe should display aggregate statistics, like the total number of reviews or an average star rating. P1 encountered both these suggestions and stated that GPT-4 \u201cfound two good ones that I would think about \u2013 with reviews and product quantity input field\u201d. P12 encountered a violation where the selected tab in the navbar was called \u201cstrategies\u201d, but the displayed page was about stock performance, which was unrelated to strategies. They commented that GPT-4 was \u201cspot on\u201d.",
                                        "sentences": [
                                            {
                                                "id": "sec-31_p1_s1",
                                                "text": "For two participants, GPT-4 made clever, high-level suggestions.",
                                                "context": "Section 5.4.4: Other Strengths",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-31_p1_s2",
                                                "text": "For instance, GPT-4 recommended that a product quantity value be changed to an editable field, so users will not have to rely on the \u2018+\u2019 or \u2018-\u2019 buttons to adjust the quantity, and cited Nielsen's \u201cFlexibility and Efficiency of Use\u201d heuristic.",
                                                "context": "Section 5.4.4: Other Strengths",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-31_p1_s3",
                                                "text": "In addition, GPT-4 suggested that a list of reviews for a cafe should display aggregate statistics, like the total number of reviews or an average star rating.",
                                                "context": "Section 5.4.4: Other Strengths",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-31_p1_s4",
                                                "text": "P1 encountered both these suggestions and stated that GPT-4 \u201cfound two good ones that I would think about \u2013 with reviews and product quantity input field\u201d.",
                                                "context": "Section 5.4.4: Other Strengths",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-31_p1_s5",
                                                "text": "P12 encountered a violation where the selected tab in the navbar was called \u201cstrategies\u201d, but the displayed page was about stock performance, which was unrelated to strategies.",
                                                "context": "Section 5.4.4: Other Strengths",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-31_p1_s6",
                                                "text": "They commented that GPT-4 was \u201cspot on\u201d.",
                                                "context": "Section 5.4.4: Other Strengths",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-32",
                                "section_number": "5.4.5",
                                "title": "Weakness 1: Overapplication of Guidelines (12/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "GPT-4 sometimes over-applied guidelines too literally, without considering the context provided by the rest of the UI, popular design conventions, nor conflicts with other guidelines. UI C in Figure 9 shows an example where the LLM correctly identified that the line thickness is inconsistent for the two tabs; however, this was a conscious design decision to show the selected tab in the broader context of the UI. P7 commented \u201cthe line under favorites indicates that we are on the favorites tab/screen; making it consistent would eliminate this distinction\u201d. Regarding popular design conventions, GPT-4 often recommended labels for common icons, like the \u2018X\u2019 and shopping bag icons, but \u201cuniversal icons do not need to be labeled\u201d, as stated by P1.",
                                        "sentences": [
                                            {
                                                "id": "sec-32_p1_s1",
                                                "text": "GPT-4 sometimes over-applied guidelines too literally, without considering the context provided by the rest of the UI, popular design conventions, nor conflicts with other guidelines.",
                                                "context": "Section 5.4.5: Weakness 1: Overapplication of Guidelines (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-32_p1_s2",
                                                "text": "UI C in Figure 9 shows an example where the LLM correctly identified that the line thickness is inconsistent for the two tabs; however, this was a conscious design decision to show the selected tab in the broader context of the UI.",
                                                "context": "Section 5.4.5: Weakness 1: Overapplication of Guidelines (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-32_p1_s3",
                                                "text": "P7 commented \u201cthe line under favorites indicates that we are on the favorites tab/screen; making it consistent would eliminate this distinction\u201d.",
                                                "context": "Section 5.4.5: Weakness 1: Overapplication of Guidelines (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-32_p1_s4",
                                                "text": "Regarding popular design conventions, GPT-4 often recommended labels for common icons, like the \u2018X\u2019 and shopping bag icons, but \u201cuniversal icons do not need to be labeled\u201d, as stated by P1.",
                                                "context": "Section 5.4.5: Weakness 1: Overapplication of Guidelines (12/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-33",
                                "section_number": "5.4.6",
                                "title": "Weakness 2: Repetition of Feedback (6/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "GPT-4 sometimes repeated the same feedback for every element of the same type. For instance, in the second UI of Figure 9, the LLM suggested increasing the spacing between the \u201cName:\u201d label and the name \u201cSuraj shakaya\u201d and then repeated this same suggestion for both \u201cContact:\u201d and \u201cLocation:\u201d. P4 commented that the LLM \u201crepeats the same type of issue, and it is especially annoying when it is incorrect.\u201d",
                                        "sentences": [
                                            {
                                                "id": "sec-33_p1_s1",
                                                "text": "GPT-4 sometimes repeated the same feedback for every element of the same type.",
                                                "context": "Section 5.4.6: Weakness 2: Repetition of Feedback (6/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-33_p1_s2",
                                                "text": "For instance, in the second UI of Figure 9, the LLM suggested increasing the spacing between the \u201cName:\u201d label and the name \u201cSuraj shakaya\u201d and then repeated this same suggestion for both \u201cContact:\u201d and \u201cLocation:\u201d.",
                                                "context": "Section 5.4.6: Weakness 2: Repetition of Feedback (6/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-33_p1_s3",
                                                "text": "P4 commented that the LLM \u201crepeats the same type of issue, and it is especially annoying when it is incorrect.\u201d",
                                                "context": "Section 5.4.6: Weakness 2: Repetition of Feedback (6/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-34",
                                "section_number": "5.4.7",
                                "title": "Weakness 3: Limitations of the JSON Representation (8/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "A limitation of using JSON to represent the UI is that GPT-4 cannot capture violations that would require processing the rendered image of a UI. For example, GPT-4 will call out issues with elements overlapping when there is no visual overlap. UI D in Figure 9 is an example where the LLM flagged an overlap issue because the bounding boxes overlap, but the photos do not overlap visually. Another error is that the LLM does not recognize center alignment for items of different sizes. This is explained by P9: \u201cThe elements are of different sizes and the bounding boxes of elements are not aligned, but the inner contents of the boxes are visually aligned.\u201d",
                                        "sentences": [
                                            {
                                                "id": "sec-34_p1_s1",
                                                "text": "A limitation of using JSON to represent the UI is that GPT-4 cannot capture violations that would require processing the rendered image of a UI.",
                                                "context": "Section 5.4.7: Weakness 3: Limitations of the JSON Representation (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-34_p1_s2",
                                                "text": "For example, GPT-4 will call out issues with elements overlapping when there is no visual overlap.",
                                                "context": "Section 5.4.7: Weakness 3: Limitations of the JSON Representation (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-34_p1_s3",
                                                "text": "UI D in Figure 9 is an example where the LLM flagged an overlap issue because the bounding boxes overlap, but the photos do not overlap visually.",
                                                "context": "Section 5.4.7: Weakness 3: Limitations of the JSON Representation (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-34_p1_s4",
                                                "text": "Another error is that the LLM does not recognize center alignment for items of different sizes.",
                                                "context": "Section 5.4.7: Weakness 3: Limitations of the JSON Representation (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-34_p1_s5",
                                                "text": "This is explained by P9: \u201cThe elements are of different sizes and the bounding boxes of elements are not aligned, but the inner contents of the boxes are visually aligned.\u201d",
                                                "context": "Section 5.4.7: Weakness 3: Limitations of the JSON Representation (8/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-35",
                                "section_number": "5.4.8",
                                "title": "Weakness 4: Vague Suggestions (5/12 Participants)",
                                "paragraphs": [
                                    {
                                        "full_text": "Five participants stated that GPT-4\u2019s suggestions were too vague, specifically regarding how to fixing the violation. P1 stated that the LLM would \u201cbe more useful if it were more specific and gave more examples on how to fix things\u201d, and P11 said they would \u201clike to see more actionable suggestions, like change this color to a specific value\u201d. The reason for this vagueness is due to our system implementation. We first attain a set of guideline violations from GPT-4 and then send the violation explanations only (without the UI) for the LLM to rephrase into constructive feedback. Since GPT-4 does not have information on the UI, it cannot suggest specific fixes in the UI to address each violation. We tried passing in the UI for the rephrasing call, but it led to high latency (several minutes), so we decided to not include the UI JSON from a usability perspective.",
                                        "sentences": [
                                            {
                                                "id": "sec-35_p1_s1",
                                                "text": "Five participants stated that GPT-4\u2019s suggestions were too vague, specifically regarding how to fixing the violation.",
                                                "context": "Section 5.4.8: Weakness 4: Vague Suggestions (5/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-35_p1_s2",
                                                "text": "P1 stated that the LLM would \u201cbe more useful if it were more specific and gave more examples on how to fix things\u201d, and P11 said they would \u201clike to see more actionable suggestions, like change this color to a specific value\u201d.",
                                                "context": "Section 5.4.8: Weakness 4: Vague Suggestions (5/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-35_p1_s3",
                                                "text": "The reason for this vagueness is due to our system implementation.",
                                                "context": "Section 5.4.8: Weakness 4: Vague Suggestions (5/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-35_p1_s4",
                                                "text": "We first attain a set of guideline violations from GPT-4 and then send the violation explanations only (without the UI) for the LLM to rephrase into constructive feedback.",
                                                "context": "Section 5.4.8: Weakness 4: Vague Suggestions (5/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-35_p1_s5",
                                                "text": "Since GPT-4 does not have information on the UI, it cannot suggest specific fixes in the UI to address each violation.",
                                                "context": "Section 5.4.8: Weakness 4: Vague Suggestions (5/12 Participants)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-35_p1_s6",
                                                "text": "We tried passing in the UI for the rephrasing call, but it led to high latency (several minutes), so we decided to not include the UI JSON from a usability perspective.",
                                                "context": "Section 5.4.8: Weakness 4: Vague Suggestions (5/12 Participants)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-36",
                                "section_number": "5.4.9",
                                "title": "Other Weaknesses",
                                "paragraphs": [
                                    {
                                        "full_text": "We noticed an interesting behavior of GPT-4 in response to designer feedback. When the LLM is notified that the guideline violation is incorrect for a group or element (from the designer hiding the suggestion), it would sometimes select a different guideline to explain the same issue. For instance, GPT-4 would often cite a violation of \u201cRecognition over Recall\u201d for unlabeled icons, but if the designer dismisses this suggestion, GPT-4 would repeat the same suggestion in the next round and cite \u201cMatch Between System and Real World\u201d instead. This behavior was encountered by 5 participants.",
                                        "sentences": [
                                            {
                                                "id": "sec-36_p1_s1",
                                                "text": "We noticed an interesting behavior of GPT-4 in response to designer feedback.",
                                                "context": "Section 5.4.9: Other Weaknesses",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-36_p1_s2",
                                                "text": "When the LLM is notified that the guideline violation is incorrect for a group or element (from the designer hiding the suggestion), it would sometimes select a different guideline to explain the same issue.",
                                                "context": "Section 5.4.9: Other Weaknesses",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-36_p1_s3",
                                                "text": "For instance, GPT-4 would often cite a violation of \u201cRecognition over Recall\u201d for unlabeled icons, but if the designer dismisses this suggestion, GPT-4 would repeat the same suggestion in the next round and cite \u201cMatch Between System and Real World\u201d instead.",
                                                "context": "Section 5.4.9: Other Weaknesses",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-36_p1_s4",
                                                "text": "This behavior was encountered by 5 participants.",
                                                "context": "Section 5.4.9: Other Weaknesses",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            }
                        ]
                    },
                    {
                        "id": "sec-37",
                        "section_number": "5.5",
                        "title": "Qualitative Results: Comparison with Human Evaluators",
                        "paragraphs": [
                            {
                                "full_text": "We used grounded theory coding and thematic analysis to characterize key qualities for 1) issues violations found by GPT-4 only, 2) found by both GPT-4 and human experts, and 3) found by human experts only. Finally, we summarize participants\u2019 feedback for the LLM from the concluding interview.",
                                "sentences": [
                                    {
                                        "id": "sec-37_p1_s1",
                                        "text": "We used grounded theory coding and thematic analysis to characterize key qualities for 1) issues violations found by GPT-4 only, 2) found by both GPT-4 and human experts, and 3) found by human experts only.",
                                        "context": "Section 5.5: Qualitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "fig10"
                                    },
                                    {
                                        "id": "sec-37_p1_s2",
                                        "text": "Finally, we summarize participants\u2019 feedback for the LLM from the concluding interview.",
                                        "context": "Section 5.5: Qualitative Results: Comparison with Human Evaluators",
                                        "associated_visual": "fig10"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": [
                            {
                                "id": "sec-38",
                                "section_number": "5.5.1",
                                "title": "Violations Found by GPT-4 only (9 percent)",
                                "paragraphs": [
                                    {
                                        "full_text": "A small fraction of the violations were found by the LLM only. Eight of the nine issues were quite subtle, covering poor text contrast, text labels that require clarification, and misalignment. The ninth suggestion missed by all participants recommended adding a new feature to filter through the cafe's reviews, and participant H8 commented \u201cthis type of suggestion did not cross my mind during the manual evaluation\u201d.",
                                        "sentences": [
                                            {
                                                "id": "sec-38_p1_s1",
                                                "text": "A small fraction of the violations were found by the LLM only.",
                                                "context": "Section 5.5.1: Violations Found by GPT-4 only (9 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-38_p1_s2",
                                                "text": "Eight of the nine issues were quite subtle, covering poor text contrast, text labels that require clarification, and misalignment.",
                                                "context": "Section 5.5.1: Violations Found by GPT-4 only (9 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-38_p1_s3",
                                                "text": "The ninth suggestion missed by all participants recommended adding a new feature to filter through the cafe's reviews, and participant H8 commented \u201cthis type of suggestion did not cross my mind during the manual evaluation\u201d.",
                                                "context": "Section 5.5.1: Violations Found by GPT-4 only (9 percent)",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-39",
                                "section_number": "5.5.2",
                                "title": "Violations Found by both Humans and GPT-4 (29 percent)",
                                "paragraphs": [
                                    {
                                        "full_text": "Twenty-nine issues were identified by both humans and the LLM. The majority of these violations (15 violations) involved text labels \u2013 labels that require clarification, and missing labels to explain an icon, element, or group. The second most common issue (6 issues) involved positioning and alignment, with four being quite obvious (e.g., the misaligned elements were scattered like in Figure 9 C). Other issues found by both include problems with the UI text (e.g., the text had too much jargon) and the hierarchical subgrouping violation shown in Figure 9 A.",
                                        "sentences": [
                                            {
                                                "id": "sec-39_p1_s1",
                                                "text": "Twenty-nine issues were identified by both humans and the LLM.",
                                                "context": "Section 5.5.2: Violations Found by both Humans and GPT-4 (29 percent)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-39_p1_s2",
                                                "text": "The majority of these violations (15 violations) involved text labels \u2013 labels that require clarification, and missing labels to explain an icon, element, or group.",
                                                "context": "Section 5.5.2: Violations Found by both Humans and GPT-4 (29 percent)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-39_p1_s3",
                                                "text": "The second most common issue (6 issues) involved positioning and alignment, with four being quite obvious (e.g., the misaligned elements were scattered like in Figure 9 C).",
                                                "context": "Section 5.5.2: Violations Found by both Humans and GPT-4 (29 percent)",
                                                "associated_visual": "fig9"
                                            },
                                            {
                                                "id": "sec-39_p1_s4",
                                                "text": "Other issues found by both include problems with the UI text (e.g., the text had too much jargon) and the hierarchical subgrouping violation shown in Figure 9 A.",
                                                "context": "Section 5.5.2: Violations Found by both Humans and GPT-4 (29 percent)",
                                                "associated_visual": "fig9"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-40",
                                "section_number": "5.5.3",
                                "title": "Violations Found by Humans only (62 percent)",
                                "paragraphs": [
                                    {
                                        "full_text": "The majority of the violations were found by human experts only (62 violations). Common characteristics of violations in this category include \u201cglobal\u201d (i.e., high-level) issues of the UI, advanced visual issues, and violations consisting of multiple distinct issues. Participants found 11 high-level issues that require understanding of the UI's purpose and context. Figure 10 (Violation H1) illustrates a global violation identified by a human expert, where a large image hides important content, and compares it to a similar, but more specific, violation found by GPT-4 (L1). While GPT-4 is able to find these \u201cglobal\u201d violations, as discussed in Sections 5.5.1 and 5.4.4, human experts found considerably more. Eight violations, found only by human experts, required advanced visual understanding of the UI. The right screenshot in Figure 10 illustrates two such violations. The tooltip (Figure 10 H3) displayed a monetary amount that not only exceeded the axes of the graph but also mismatched the graph's content about sleep duration. Violation H2 highlighted redundant links to the user profile with via both a profile image and a profile icon. Finally, a few participants stated that parts of the UI shown in Figure 11 (Round 1) had clashing visual design and an overly complicated background.",
                                        "sentences": [
                                            {
                                                "id": "sec-40_p1_s1",
                                                "text": "The majority of the violations were found by human experts only (62 violations).",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s2",
                                                "text": "Common characteristics of violations in this category include \u201cglobal\u201d (i.e., high-level) issues of the UI, advanced visual issues, and violations consisting of multiple distinct issues.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s3",
                                                "text": "Participants found 11 high-level issues that require understanding of the UI's purpose and context.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s4",
                                                "text": "Figure 10 (Violation H1) illustrates a global violation identified by a human expert, where a large image hides important content, and compares it to a similar, but more specific, violation found by GPT-4 (L1).",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s5",
                                                "text": "While GPT-4 is able to find these \u201cglobal\u201d violations, as discussed in Sections 5.5.1 and 5.4.4, human experts found considerably more.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s6",
                                                "text": "Eight violations, found only by human experts, required advanced visual understanding of the UI.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s7",
                                                "text": "The right screenshot in Figure 10 illustrates two such violations.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s8",
                                                "text": "The tooltip (Figure 10 H3) displayed a monetary amount that not only exceeded the axes of the graph but also mismatched the graph's content about sleep duration.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s9",
                                                "text": "Violation H2 highlighted redundant links to the user profile with via both a profile image and a profile icon.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-40_p1_s10",
                                                "text": "Finally, a few participants stated that parts of the UI shown in Figure 11 (Round 1) had clashing visual design and an overly complicated background.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    },
                                    {
                                        "full_text": "Ten of the violations recorded by the participants involved combinations of several distinct issues for a single group or element. For instance, a violation for the UI in Figure 11 (Round 1) stated that \u201cThe title has incorrect spelling and grammar, is not aligned on the page/has awkward margins, has inconsistent text styles for the same sentence, and includes clashing visual elements\u201d. Finally, participants found 22 issues that were similar to the types of issues caught by GPT-4, such as misalignment, unclear labels, and redundancy. This implies that GPT-4 is less comprehensive than a group of 6 human experts, as each UI was evaluated by 6 study participants.",
                                        "sentences": [
                                            {
                                                "id": "sec-40_p2_s1",
                                                "text": "Ten of the violations recorded by the participants involved combinations of several distinct issues for a single group or element.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-40_p2_s2",
                                                "text": "For instance, a violation for the UI in Figure 11 (Round 1) stated that \u201cThe title has incorrect spelling and grammar, is not aligned on the page/has awkward margins, has inconsistent text styles for the same sentence, and includes clashing visual elements\u201d.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-40_p2_s3",
                                                "text": "Finally, participants found 22 issues that were similar to the types of issues caught by GPT-4, such as misalignment, unclear labels, and redundancy.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-40_p2_s4",
                                                "text": "This implies that GPT-4 is less comprehensive than a group of 6 human experts, as each UI was evaluated by 6 study participants.",
                                                "context": "Section 5.5.3: Violations Found by Humans only (62 percent)",
                                                "associated_visual": "fig11"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-41",
                                "section_number": "5.5.4",
                                "title": "Interview Findings",
                                "paragraphs": [
                                    {
                                        "full_text": "Participants were generally impressed by the convenience of this plugin, which could find helpful guideline violations at a much faster speed than manual evaluation. H6 said that it \u201ccan cut about 50 percent of your work, and is at the level of a good junior designer\u201d, and H3 said they \u201cwish it was already out for use\u201d. Compared to the violations they found manually, several participants said GPT-4 was more thorough and detailed (H3, H4, H5, H6, H10). H1 \u201cappreciated how the LLM could find subtle violations that were missed\u201d, and P5 said they were \u201coverwhelmed by the number of issues in some UIs\u201d and appreciated how the LLM can catch violations that were \u201ctedious to find\u201d. H6 said GPT-4 \u201cgoes into a much lower level of resolution than is commercially feasible to do, since it takes a long time\u201d. H1, H3, and H9 valued how GPT-4 could sometimes better articulate the violation. H9 stated that they were \u201cpleasantly surprised at how it picked the right way to describe the problem\u201d, regarding an issue they struggled with describing. Finally, H1, H2, H4, H7, and H8 all appreciated how GPT-4 found violations that were missed during their manual evaluation. H7 said \u201cit was useful, as it captured more cases than I found\u201d.",
                                        "sentences": [
                                            {
                                                "id": "sec-41_p1_s1",
                                                "text": "Participants were generally impressed by the convenience of this plugin, which could find helpful guideline violations at a much faster speed than manual evaluation.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s2",
                                                "text": "H6 said that it \u201ccan cut about 50 percent of your work, and is at the level of a good junior designer\u201d, and H3 said they \u201cwish it was already out for use\u201d.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s3",
                                                "text": "Compared to the violations they found manually, several participants said GPT-4 was more thorough and detailed (H3, H4, H5, H6, H10).",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s4",
                                                "text": "H1 \u201cappreciated how the LLM could find subtle violations that were missed\u201d, and P5 said they were \u201coverwhelmed by the number of issues in some UIs\u201d and appreciated how the LLM can catch violations that were \u201ctedious to find\u201d.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s5",
                                                "text": "H6 said GPT-4 \u201cgoes into a much lower level of resolution than is commercially feasible to do, since it takes a long time\u201d.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s6",
                                                "text": "H1, H3, and H9 valued how GPT-4 could sometimes better articulate the violation.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s7",
                                                "text": "H9 stated that they were \u201cpleasantly surprised at how it picked the right way to describe the problem\u201d, regarding an issue they struggled with describing.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s8",
                                                "text": "Finally, H1, H2, H4, H7, and H8 all appreciated how GPT-4 found violations that were missed during their manual evaluation.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p1_s9",
                                                "text": "H7 said \u201cit was useful, as it captured more cases than I found\u201d.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    },
                                    {
                                        "full_text": "Participants brought up weaknesses of GPT-4\u2019s feedback, which mostly aligned with the findings in Sections 5.4 and 5.5.3. These limitations include missing the majority of the \u201cglobal\u201d violations (H1, H5, H9), limited visual understanding of the UI (H2, H8, H11), and poor knowledge of popular design conventions (H2, H7, H8, H10, H11). Finally, like the participants in the Usage study, those in this study also did not consider the LLM's mistakes to be a significant issue. H10 said \u201cif the feedback is correct, then is it very helpful, and if not, it is not a big deal as you can just dismiss it\u201d, and H6 said \u201cthe 60 percent success rate is not a problem, as it saves a lot of time in the end\u201d.",
                                        "sentences": [
                                            {
                                                "id": "sec-41_p2_s1",
                                                "text": "Participants brought up weaknesses of GPT-4\u2019s feedback, which mostly aligned with the findings in Sections 5.4 and 5.5.3.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p2_s2",
                                                "text": "These limitations include missing the majority of the \u201cglobal\u201d violations (H1, H5, H9), limited visual understanding of the UI (H2, H8, H11), and poor knowledge of popular design conventions (H2, H7, H8, H10, H11).",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p2_s3",
                                                "text": "Finally, like the participants in the Usage study, those in this study also did not consider the LLM's mistakes to be a significant issue.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-41_p2_s4",
                                                "text": "H10 said \u201cif the feedback is correct, then is it very helpful, and if not, it is not a big deal as you can just dismiss it\u201d, and H6 said \u201cthe 60 percent success rate is not a problem, as it saves a lot of time in the end\u201d.",
                                                "context": "Section 5.5.4: Interview Findings",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            }
                        ]
                    },
                    {
                        "id": "sec-42",
                        "section_number": "5.6",
                        "title": "Qualitative Results: Integration into Existing Design Practices",
                        "paragraphs": [
                            {
                                "full_text": "We analyzed the interview responses from the Usage study with grounded theory coding and thematic analysis to determine this tool's fit into existing design practice. The emerging themes centered around how and when designers would integrate it in their practice, potential broader use cases, and possible dangers of an imperfect tool.",
                                "sentences": [
                                    {
                                        "id": "sec-42_p1_s1",
                                        "text": "We analyzed the interview responses from the Usage study with grounded theory coding and thematic analysis to determine this tool's fit into existing design practice.",
                                        "context": "Section 5.6: Qualitative Results: Integration into Existing Design Practices",
                                        "associated_visual": "fig10"
                                    },
                                    {
                                        "id": "sec-42_p1_s2",
                                        "text": "The emerging themes centered around how and when designers would integrate it in their practice, potential broader use cases, and possible dangers of an imperfect tool.",
                                        "context": "Section 5.6: Qualitative Results: Integration into Existing Design Practices",
                                        "associated_visual": "fig10"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": [
                            {
                                "id": "sec-43",
                                "section_number": "5.6.1",
                                "title": "How and When Designers Would Integrate this Tool in Practice",
                                "paragraphs": [
                                    {
                                        "full_text": "Nine out of 12 participants said they would use this tool. The three who would not cited inaccurate and vague suggestions as reasons. Of the nine participants who would use this tool, three said they would use it during the initial stages of the design. This includes tasks like determining the hierarchy of components in low-fidelity prototypes (P5, P7) and \u201cexploring early stage concepts\u201d (P7). The other six participants would use this tool in the later design stages. P5 said they would also use it \u201cafter the first draft of high fidelity for alignment issues\u201d. P11 said \u201cafter finalizing their design, I would run a check before sending it to engineering\u201d, and they would also run this tool \u201cafter making large design changes to see if any design considerations were missed\u201d. Finally, P6 said they would run this tool \u201cafter prototyping and before usability testing; it is valuable to test the LLM's recommendations during the user test\u201d.",
                                        "sentences": [
                                            {
                                                "id": "sec-43_p1_s1",
                                                "text": "Nine out of 12 participants said they would use this tool.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s2",
                                                "text": "The three who would not cited inaccurate and vague suggestions as reasons.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s3",
                                                "text": "Of the nine participants who would use this tool, three said they would use it during the initial stages of the design.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s4",
                                                "text": "This includes tasks like determining the hierarchy of components in low-fidelity prototypes (P5, P7) and \u201cexploring early stage concepts\u201d (P7).",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s5",
                                                "text": "The other six participants would use this tool in the later design stages.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s6",
                                                "text": "P5 said they would also use it \u201cafter the first draft of high fidelity for alignment issues\u201d.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s7",
                                                "text": "P11 said \u201cafter finalizing their design, I would run a check before sending it to engineering\u201d, and they would also run this tool \u201cafter making large design changes to see if any design considerations were missed\u201d.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p1_s8",
                                                "text": "Finally, P6 said they would run this tool \u201cafter prototyping and before usability testing; it is valuable to test the LLM's recommendations during the user test\u201d.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    },
                                    {
                                        "full_text": "Regarding iterative or one-shot usage, seven participants preferred using this tool for a single check, and the other five preferred iterative usage. The participants who preferred one-shot usage stated that the LLM feedback was less accurate and helpful in later rounds (P4, P5, P7), found that the \u201cfirst round of edits was sufficient\u201d (P8), or preferred to use the LLM's suggestions as \u201ctips to start off the thinking process, but not rely on it\u201d (P7).",
                                        "sentences": [
                                            {
                                                "id": "sec-43_p2_s1",
                                                "text": "Regarding iterative or one-shot usage, seven participants preferred using this tool for a single check, and the other five preferred iterative usage.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-43_p2_s2",
                                                "text": "The participants who preferred one-shot usage stated that the LLM feedback was less accurate and helpful in later rounds (P4, P5, P7), found that the \u201cfirst round of edits was sufficient\u201d (P8), or preferred to use the LLM's suggestions as \u201ctips to start off the thinking process, but not rely on it\u201d (P7).",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    },
                                    {
                                        "full_text": "Participants also had positive feedback on the interaction design of the plugin. Six participants stated that they liked being able to click on the group or element's name in the suggestion to select it in the mockup (Figure 2, A). Four participants stated they like the references to guidelines, as it \u201cadds credibility\u201d (P3), and they could reference the guideline's source material for more context (P9). P3 liked the constructive framing of the violations, saying that \u201cthe type of language is encouraging, which is nice to see\u201d. Finally, P4 liked the \u201cbuilt-in guideline options and the flexibility to specify my own guidelines\u201d.",
                                        "sentences": [
                                            {
                                                "id": "sec-43_p3_s1",
                                                "text": "Participants also had positive feedback on the interaction design of the plugin.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig2"
                                            },
                                            {
                                                "id": "sec-43_p3_s2",
                                                "text": "Six participants stated that they liked being able to click on the group or element's name in the suggestion to select it in the mockup (Figure 2, A).",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig2"
                                            },
                                            {
                                                "id": "sec-43_p3_s3",
                                                "text": "Four participants stated they like the references to guidelines, as it \u201cadds credibility\u201d (P3), and they could reference the guideline's source material for more context (P9).",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig2"
                                            },
                                            {
                                                "id": "sec-43_p3_s4",
                                                "text": "P3 liked the constructive framing of the violations, saying that \u201cthe type of language is encouraging, which is nice to see\u201d.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig2"
                                            },
                                            {
                                                "id": "sec-43_p3_s5",
                                                "text": "Finally, P4 liked the \u201cbuilt-in guideline options and the flexibility to specify my own guidelines\u201d.",
                                                "context": "Section 5.6.1: How and When Designers Would Integrate this Tool in Practice",
                                                "associated_visual": "fig2"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-44",
                                "section_number": "5.6.2",
                                "title": "Potential Broader Use Cases",
                                "paragraphs": [
                                    {
                                        "full_text": "The participants brainstormed a long list of use cases for this tool. Some use cases are based on real observations from the Usage study, and others are hypothetical (from speculation). For observed use cases, the most common was using this tool to save busywork, which was mentioned by 5 participants. P10 said this tool would \u201csave time on mundane tasks\u201d, for things like \u201cspellcheck\u201d, and 3 participants said it can catch small details like line spacing, size, and \u201csmall things you might miss\u201d (P11). Other observed use cases include evaluating other people's designs during teamwork (P3) and secondary research (P8), and using it \u201cas a first round of usability testing\u201d (P8). Common hypothetical use cases include using it for accessibility checks (4 participants) and training novices (10 participants). P5 stated that this tool can \u201chelp younger designers learn and reinforce these guidelines\u201d, and P11 and P12 said the mistakes made by the LLM can train novices to carefully consider design suggestions and be more skeptical. Other speculative use cases include getting a second opinion on your UIs (for designers working solo) (P6), checking for compliance with brand standards (P10) and company rules (P12), and \u201clarge-scale evaluation, where you designed a lot of screens and want a quick evaluation\u201d (P9).",
                                        "sentences": [
                                            {
                                                "id": "sec-44_p1_s1",
                                                "text": "The participants brainstormed a long list of use cases for this tool.",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s2",
                                                "text": "Some use cases are based on real observations from the Usage study, and others are hypothetical (from speculation).",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s3",
                                                "text": "For observed use cases, the most common was using this tool to save busywork, which was mentioned by 5 participants.",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s4",
                                                "text": "P10 said this tool would \u201csave time on mundane tasks\u201d, for things like \u201cspellcheck\u201d, and 3 participants said it can catch small details like line spacing, size, and \u201csmall things you might miss\u201d (P11).",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s5",
                                                "text": "Other observed use cases include evaluating other people's designs during teamwork (P3) and secondary research (P8), and using it \u201cas a first round of usability testing\u201d (P8).",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s6",
                                                "text": "Common hypothetical use cases include using it for accessibility checks (4 participants) and training novices (10 participants).",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s7",
                                                "text": "P5 stated that this tool can \u201chelp younger designers learn and reinforce these guidelines\u201d, and P11 and P12 said the mistakes made by the LLM can train novices to carefully consider design suggestions and be more skeptical.",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-44_p1_s8",
                                                "text": "Other speculative use cases include getting a second opinion on your UIs (for designers working solo) (P6), checking for compliance with brand standards (P10) and company rules (P12), and \u201clarge-scale evaluation, where you designed a lot of screens and want a quick evaluation\u201d (P9).",
                                                "context": "Section 5.6.2: Potential Broader Use Cases",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-45",
                                "section_number": "5.6.3",
                                "title": "Potential Dangers of this Tool",
                                "paragraphs": [
                                    {
                                        "full_text": "Although the tool sometimes reported inaccurate violations, most participants (8 out of 12) considered the tool not dangerous because there is a human in the loop to catch errors. However, some potential dangers stated by participants include \u201cthinking there is something wrong with the design when there might not be\u201d (P2) and users who trust the LLM 100 percent (P1, P4, P6, P9). For instance, novices may fully follow the LLM's advice (P1, P2, P3, P6, P11), and P3 suggested that they could use this tool with expert supervision. However, 4 participants felt that novices should be able to detect the LLM's mistakes. P3 stated that \u201cthe wrong suggestions are outlandish enough that novices can tell that it does not make sense\u201d. There were also some negative feedback regarding the plugin design. Three participants (P2, P8, P11) found the LLM suggestions too wordy, and P7 did not like how the tool \u201ccan only evaluate one screen at a time, as opposed to the whole flow.\u201d",
                                        "sentences": [
                                            {
                                                "id": "sec-45_p1_s1",
                                                "text": "Although the tool sometimes reported inaccurate violations, most participants (8 out of 12) considered the tool not dangerous because there is a human in the loop to catch errors.",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-45_p1_s2",
                                                "text": "However, some potential dangers stated by participants include \u201cthinking there is something wrong with the design when there might not be\u201d (P2) and users who trust the LLM 100 percent (P1, P4, P6, P9).",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-45_p1_s3",
                                                "text": "For instance, novices may fully follow the LLM's advice (P1, P2, P3, P6, P11), and P3 suggested that they could use this tool with expert supervision.",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-45_p1_s4",
                                                "text": "However, 4 participants felt that novices should be able to detect the LLM's mistakes.",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-45_p1_s5",
                                                "text": "P3 stated that \u201cthe wrong suggestions are outlandish enough that novices can tell that it does not make sense\u201d.",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-45_p1_s6",
                                                "text": "There were also some negative feedback regarding the plugin design.",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            },
                                            {
                                                "id": "sec-45_p1_s7",
                                                "text": "Three participants (P2, P8, P11) found the LLM suggestions too wordy, and P7 did not like how the tool \u201ccan only evaluate one screen at a time, as opposed to the whole flow.\u201d",
                                                "context": "Section 5.6.3: Potential Dangers of this Tool",
                                                "associated_visual": "fig10"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            }
                        ]
                    }
                ]
            },
            {
                "id": "sec-46",
                "section_number": "6",
                "title": "DISCUSSION",
                "paragraphs": [
                    {
                        "full_text": "We explored the potential of using LLMs for automated heuristic evaluation. In particular, we assessed the feasibility of the best performing LLM (GPT-4) and how a tool built on this can fit into existing design practice. We discuss interesting insights from our findings, and implications for its feasibility and integration into design practice.",
                        "sentences": [
                            {
                                "id": "sec-46_p1_s1",
                                "text": "We explored the potential of using LLMs for automated heuristic evaluation.",
                                "context": "Section 6: DISCUSSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-46_p1_s2",
                                "text": "In particular, we assessed the feasibility of the best performing LLM (GPT-4) and how a tool built on this can fit into existing design practice.",
                                "context": "Section 6: DISCUSSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-46_p1_s3",
                                "text": "We discuss interesting insights from our findings, and implications for its feasibility and integration into design practice.",
                                "context": "Section 6: DISCUSSION",
                                "associated_visual": "fig11"
                            }
                        ]
                    }
                ],
                "lists": [],
                "subsections": [
                    {
                        "id": "sec-47",
                        "section_number": "6.1",
                        "title": "Feasibility of GPT-4 for Heuristic Evaluation",
                        "paragraphs": [
                            {
                                "full_text": "We assessed GPT-4\u2019s performance quantitatively and qualitatively. From our qualitative analysis, we identified a concrete set of strengths and weaknesses. Most of these weaknesses could potentially be addressed, which we will discuss in Future Work (Section 7). Quantitatively, the GPT-4 was judged to be more accurate and helpful than not during the Performance study and the first round of the Usage study. The authors selected the UIs for both studies because they identified issues according to the chosen guidelines. This suggests that the LLM can identify some weaknesses in poor UI designs. The decrease in performance after each iteration during the Usage study could be explained by the fact that the participants are design experts, so their edits likely improved the UI overall, leaving fewer guideline violations for GPT-4 to detect. As a result, the task becomes harder for the LLM, causing it to detect erroneous violations. Figure 11 illustrates this; GPT-4\u2019s suggestions become less accurate and helpful as P2 iterates on the UI using the CrowdCrit guidelines, while the UI's visual design becomes noticeably better going into each round of evaluation. This general decrease in performance as the UI improves implies that this tool is perhaps not yet suitable for iterative usage by expert designers. In addition to being affected by the quality of the UI mockup, the performance also varies depending on the guidelines used.",
                                "sentences": [
                                    {
                                        "id": "sec-47_p1_s1",
                                        "text": "We assessed GPT-4\u2019s performance quantitatively and qualitatively.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s2",
                                        "text": "From our qualitative analysis, we identified a concrete set of strengths and weaknesses.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s3",
                                        "text": "Most of these weaknesses could potentially be addressed, which we will discuss in Future Work (Section 7).",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s4",
                                        "text": "Quantitatively, the GPT-4 was judged to be more accurate and helpful than not during the Performance study and the first round of the Usage study.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s5",
                                        "text": "The authors selected the UIs for both studies because they identified issues according to the chosen guidelines.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s6",
                                        "text": "This suggests that the LLM can identify some weaknesses in poor UI designs.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s7",
                                        "text": "The decrease in performance after each iteration during the Usage study could be explained by the fact that the participants are design experts, so their edits likely improved the UI overall, leaving fewer guideline violations for GPT-4 to detect.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s8",
                                        "text": "As a result, the task becomes harder for the LLM, causing it to detect erroneous violations.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s9",
                                        "text": "Figure 11 illustrates this; GPT-4\u2019s suggestions become less accurate and helpful as P2 iterates on the UI using the CrowdCrit guidelines, while the UI's visual design becomes noticeably better going into each round of evaluation.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s10",
                                        "text": "This general decrease in performance as the UI improves implies that this tool is perhaps not yet suitable for iterative usage by expert designers.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-47_p1_s11",
                                        "text": "In addition to being affected by the quality of the UI mockup, the performance also varies depending on the guidelines used.",
                                        "context": "Section 6.1: Feasibility of GPT-4 for Heuristic Evaluation",
                                        "associated_visual": "fig11"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": [
                            {
                                "id": "sec-48",
                                "section_number": "6.1.1",
                                "title": "CrowdCrit",
                                "paragraphs": [
                                    {
                                        "full_text": "GPT-4 performed the best with CrowdCrit's visual design heuristics in both accuracy and helpfulness. This is likely due to the prevalence of specific visual design heuristics that rely on mathematical checks for alignment, spacing, and consistency in size \u2013 all checks that are straightforward to compute with layout information found in the UI JSON (except for center alignment and overlap). CrowdCrit also covers UI text accuracy (i.e., grammar and spelling), where LLMs excel in identifying and fixing issues. Regarding helpfulness, many visual design errors are subtle (like slight misalignment), and participants found it very useful when the LLM found them.",
                                        "sentences": [
                                            {
                                                "id": "sec-48_p1_s1",
                                                "text": "GPT-4 performed the best with CrowdCrit's visual design heuristics in both accuracy and helpfulness.",
                                                "context": "Section 6.1.1: CrowdCrit",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-48_p1_s2",
                                                "text": "This is likely due to the prevalence of specific visual design heuristics that rely on mathematical checks for alignment, spacing, and consistency in size \u2013 all checks that are straightforward to compute with layout information found in the UI JSON (except for center alignment and overlap).",
                                                "context": "Section 6.1.1: CrowdCrit",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-48_p1_s3",
                                                "text": "CrowdCrit also covers UI text accuracy (i.e., grammar and spelling), where LLMs excel in identifying and fixing issues.",
                                                "context": "Section 6.1.1: CrowdCrit",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-48_p1_s4",
                                                "text": "Regarding helpfulness, many visual design errors are subtle (like slight misalignment), and participants found it very useful when the LLM found them.",
                                                "context": "Section 6.1.1: CrowdCrit",
                                                "associated_visual": "fig11"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-49",
                                "section_number": "6.1.2",
                                "title": "Nielsen Norman 10 Usability Heuristics",
                                "paragraphs": [
                                    {
                                        "full_text": "We expected the Nielsen Norman 10 Usability Heuristics to have the best performance, as they are widespread on the web and would have appeared in GPT-4\u2019s training data. However, its worse performance, especially for helpfulness, could be because many of its heuristics apply to interactions or flows (like \u201cUser Control and Freedom\u201d, \u201cHelp and Documentation\u201d, and \u201cError Prevention\u201d) that are out of scope for single-screen, static mock-ups. In addition, the \u201cAesthetic and Minimalist Design\u201d heuristic covers visual design but had poor performance. This is probably because this heuristic is quite vague, focusing on only presenting necessary information to the user; this is difficult to assess, especially without the context provided by other screens in the flow. On the other hand, GPT-4 performed well on \u201cConsistency and Standards\u201d, which mostly checked for consistency in the visual layout, like size and alignment, which are straightforward numerical checks.",
                                        "sentences": [
                                            {
                                                "id": "sec-49_p1_s1",
                                                "text": "We expected the Nielsen Norman 10 Usability Heuristics to have the best performance, as they are widespread on the web and would have appeared in GPT-4\u2019s training data.",
                                                "context": "Section 6.1.2: Nielsen Norman 10 Usability Heuristics",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-49_p1_s2",
                                                "text": "However, its worse performance, especially for helpfulness, could be because many of its heuristics apply to interactions or flows (like \u201cUser Control and Freedom\u201d, \u201cHelp and Documentation\u201d, and \u201cError Prevention\u201d) that are out of scope for single-screen, static mock-ups.",
                                                "context": "Section 6.1.2: Nielsen Norman 10 Usability Heuristics",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-49_p1_s3",
                                                "text": "In addition, the \u201cAesthetic and Minimalist Design\u201d heuristic covers visual design but had poor performance.",
                                                "context": "Section 6.1.2: Nielsen Norman 10 Usability Heuristics",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-49_p1_s4",
                                                "text": "This is probably because this heuristic is quite vague, focusing on only presenting necessary information to the user; this is difficult to assess, especially without the context provided by other screens in the flow.",
                                                "context": "Section 6.1.2: Nielsen Norman 10 Usability Heuristics",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-49_p1_s5",
                                                "text": "On the other hand, GPT-4 performed well on \u201cConsistency and Standards\u201d, which mostly checked for consistency in the visual layout, like size and alignment, which are straightforward numerical checks.",
                                                "context": "Section 6.1.2: Nielsen Norman 10 Usability Heuristics",
                                                "associated_visual": "fig11"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            },
                            {
                                "id": "sec-50",
                                "section_number": "6.1.3",
                                "title": "Semantic Grouping",
                                "paragraphs": [
                                    {
                                        "full_text": "The lack of training data on Semantic Grouping guidelines, which were recently developed, could explain their lower accuracy. In addition, the quality of group/element names in the UI JSON impacts the assessment of the UI's semantic organization. Since designers must manually add these names, they may not always be accurate. While we can automate the naming of groups, they are determined from the labels of their members. Many leaf elements, like icons and images, are also missing labels, which may affect the accuracy of the automatically generated labels. While GPT-4 generally performed well for evaluating the relatedness of elements, as mentioned by study participants, the times it did not perform well for semantic grouping were probably for poorly annotated UIs.",
                                        "sentences": [
                                            {
                                                "id": "sec-50_p1_s1",
                                                "text": "The lack of training data on Semantic Grouping guidelines, which were recently developed, could explain their lower accuracy.",
                                                "context": "Section 6.1.3: Semantic Grouping",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-50_p1_s2",
                                                "text": "In addition, the quality of group/element names in the UI JSON impacts the assessment of the UI's semantic organization.",
                                                "context": "Section 6.1.3: Semantic Grouping",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-50_p1_s3",
                                                "text": "Since designers must manually add these names, they may not always be accurate.",
                                                "context": "Section 6.1.3: Semantic Grouping",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-50_p1_s4",
                                                "text": "While we can automate the naming of groups, they are determined from the labels of their members.",
                                                "context": "Section 6.1.3: Semantic Grouping",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-50_p1_s5",
                                                "text": "Many leaf elements, like icons and images, are also missing labels, which may affect the accuracy of the automatically generated labels.",
                                                "context": "Section 6.1.3: Semantic Grouping",
                                                "associated_visual": "fig11"
                                            },
                                            {
                                                "id": "sec-50_p1_s6",
                                                "text": "While GPT-4 generally performed well for evaluating the relatedness of elements, as mentioned by study participants, the times it did not perform well for semantic grouping were probably for poorly annotated UIs.",
                                                "context": "Section 6.1.3: Semantic Grouping",
                                                "associated_visual": "fig11"
                                            }
                                        ]
                                    }
                                ],
                                "lists": [],
                                "subsections": []
                            }
                        ]
                    },
                    {
                        "id": "sec-51",
                        "section_number": "6.2",
                        "title": "General Insights into LLMs and their Future Development",
                        "paragraphs": [
                            {
                                "full_text": "The LLM comparison analysis revealed that this automated heuristic evaluation task of UIs in JSON form is hard enough to require the largest state-of-the-art model with the strongest reasoning skills (GPT-4). The other state-of-the-art models were smaller and struggled with this task; they either missed most of the violations or indiscriminately applied the guidelines and had difficulty following the detailed instructions. While these LLMs found some helpful violations, their performance falls short for building a system. The prompt exploration study showed that breaking tasks into smaller, simpler ones and providing the maximum amount of guidance yielded the strongest performance for GPT-4, and this insight probably applies to other LLMs.",
                                "sentences": [
                                    {
                                        "id": "sec-51_p1_s1",
                                        "text": "The LLM comparison analysis revealed that this automated heuristic evaluation task of UIs in JSON form is hard enough to require the largest state-of-the-art model with the strongest reasoning skills (GPT-4).",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-51_p1_s2",
                                        "text": "The other state-of-the-art models were smaller and struggled with this task; they either missed most of the violations or indiscriminately applied the guidelines and had difficulty following the detailed instructions.",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-51_p1_s3",
                                        "text": "While these LLMs found some helpful violations, their performance falls short for building a system.",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-51_p1_s4",
                                        "text": "The prompt exploration study showed that breaking tasks into smaller, simpler ones and providing the maximum amount of guidance yielded the strongest performance for GPT-4, and this insight probably applies to other LLMs.",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    }
                                ]
                            },
                            {
                                "full_text": "Furthermore, LLMs are rapidly advancing. Recent developments include multimodal models that can accept images as input (e.g., GPT-4V), and models with much larger context windows (e.g., GPT-4-Turbo). Multimodal LLMs could accept UI screenshots, which may give the model a better visual understanding of the UI; larger context windows would enable more complex UIs, an extensive list of few shot examples, or chain-of-thought prompting. These developments could address some of the limitations we discussed and may improve LLM performance for this task; however, this would have to be evaluated in future work, as our experiments with earlier multimodal models were not successful.",
                                "sentences": [
                                    {
                                        "id": "sec-51_p2_s1",
                                        "text": "Furthermore, LLMs are rapidly advancing.",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-51_p2_s2",
                                        "text": "Recent developments include multimodal models that can accept images as input (e.g., GPT-4V), and models with much larger context windows (e.g., GPT-4-Turbo).",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-51_p2_s3",
                                        "text": "Multimodal LLMs could accept UI screenshots, which may give the model a better visual understanding of the UI; larger context windows would enable more complex UIs, an extensive list of few shot examples, or chain-of-thought prompting.",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-51_p2_s4",
                                        "text": "These developments could address some of the limitations we discussed and may improve LLM performance for this task; however, this would have to be evaluated in future work, as our experiments with earlier multimodal models were not successful.",
                                        "context": "Section 6.2: General Insights into LLMs and their Future Development",
                                        "associated_visual": "fig11"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-52",
                        "section_number": "6.3",
                        "title": "Comparison with Human Evaluators",
                        "paragraphs": [
                            {
                                "full_text": "While GPT-4 and a human evaluator had comparable F1 scores, human evaluators had higher precision, a better understanding of the UI's context, found more \u201cglobal\u201d (i.e., high-level) violations, and had superior visual understanding of the UI. GPT-4, on the other hand, was more thorough, detailed, and specific, catching a greater number of helpful issues than an individual human evaluator. Furthermore, GPT-4 was better at catching detailed errors that were tedious for humans to find and was also better at finding subtle violations. These findings imply that the strengths of humans and GPT-4 are complementary for heuristic evaluation.",
                                "sentences": [
                                    {
                                        "id": "sec-52_p1_s1",
                                        "text": "While GPT-4 and a human evaluator had comparable F1 scores, human evaluators had higher precision, a better understanding of the UI's context, found more \u201cglobal\u201d (i.e., high-level) violations, and had superior visual understanding of the UI.",
                                        "context": "Section 6.3: Comparison with Human Evaluators",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-52_p1_s2",
                                        "text": "GPT-4, on the other hand, was more thorough, detailed, and specific, catching a greater number of helpful issues than an individual human evaluator.",
                                        "context": "Section 6.3: Comparison with Human Evaluators",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-52_p1_s3",
                                        "text": "Furthermore, GPT-4 was better at catching detailed errors that were tedious for humans to find and was also better at finding subtle violations.",
                                        "context": "Section 6.3: Comparison with Human Evaluators",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-52_p1_s4",
                                        "text": "These findings imply that the strengths of humans and GPT-4 are complementary for heuristic evaluation.",
                                        "context": "Section 6.3: Comparison with Human Evaluators",
                                        "associated_visual": "fig11"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    },
                    {
                        "id": "sec-53",
                        "section_number": "6.4",
                        "title": "Fit into Design Practice",
                        "paragraphs": [
                            {
                                "full_text": "We found that even with its current performance, participants were generally positive towards this tool and would use it in their practice. They stated that the errors made by GPT-4 were easy to detect and not dangerous, as there is a human in the loop to ensure that updates to the UI mockup are based only on valid feedback. Given the LLM's strengths, designers have brought up various use cases for this plugin, such as finding subtle visual design errors in their high-fidelity mockups and planning the grouping structure of their low-fidelity designs. The core capabilities of this plugin allow designers to evaluate their Figma mockup against any set of guidelines, and participants suggested extending it to check for accessibility and compliance with company/brand standards. The experts\u2019 acceptance of the LLM's imperfect suggestions, combined with numerous suggested use cases, implies that an automated LLM-driven heuristic evaluation tool may soon find a place in design practice, supporting human designers with various aspects of the design process.",
                                "sentences": [
                                    {
                                        "id": "sec-53_p1_s1",
                                        "text": "We found that even with its current performance, participants were generally positive towards this tool and would use it in their practice.",
                                        "context": "Section 6.4: Fit into Design Practice",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-53_p1_s2",
                                        "text": "They stated that the errors made by GPT-4 were easy to detect and not dangerous, as there is a human in the loop to ensure that updates to the UI mockup are based only on valid feedback.",
                                        "context": "Section 6.4: Fit into Design Practice",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-53_p1_s3",
                                        "text": "Given the LLM's strengths, designers have brought up various use cases for this plugin, such as finding subtle visual design errors in their high-fidelity mockups and planning the grouping structure of their low-fidelity designs.",
                                        "context": "Section 6.4: Fit into Design Practice",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-53_p1_s4",
                                        "text": "The core capabilities of this plugin allow designers to evaluate their Figma mockup against any set of guidelines, and participants suggested extending it to check for accessibility and compliance with company/brand standards.",
                                        "context": "Section 6.4: Fit into Design Practice",
                                        "associated_visual": "fig11"
                                    },
                                    {
                                        "id": "sec-53_p1_s5",
                                        "text": "The experts\u2019 acceptance of the LLM's imperfect suggestions, combined with numerous suggested use cases, implies that an automated LLM-driven heuristic evaluation tool may soon find a place in design practice, supporting human designers with various aspects of the design process.",
                                        "context": "Section 6.4: Fit into Design Practice",
                                        "associated_visual": "fig11"
                                    }
                                ]
                            }
                        ],
                        "lists": [],
                        "subsections": []
                    }
                ]
            },
            {
                "id": "sec-54",
                "section_number": "7",
                "title": "LIMITATIONS AND FUTURE WORK",
                "paragraphs": [
                    {
                        "full_text": "There are several limitations with this system and studies. For the system, the LLM's accuracy on semantic heuristics depends on the quality of the names designers manually add to elements. Furthermore, due to context window limitations (8.1k tokens for GPT-4), the plugin could only evaluate one static mobile UI screen at a time. This prevents the plugin from evaluating interactivity, design consistency across screens, and task flows. Larger UIs, like websites and desktop apps, are more complex and would also exceed this context window size. While there are models with larger context windows (Claude 2 and GPT-3.5-16k), they did not produce sufficiently helpful output in our exploration. Other LLMs with even smaller context windows, such as Llama 2 and GPT-3.5-turbo, are not suitable for realistic UIs. Regarding the studies, they captured the quality of the LLM's feedback, and participants used the plugin to revise mostly high-fidelity mockups. These studies did not capture the impact of this plugin's usage on a more realistic design scenario, where designers start with an idea and design a UI mockup based on it.",
                        "sentences": [
                            {
                                "id": "sec-54_p1_s1",
                                "text": "There are several limitations with this system and studies.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s2",
                                "text": "For the system, the LLM's accuracy on semantic heuristics depends on the quality of the names designers manually add to elements.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s3",
                                "text": "Furthermore, due to context window limitations (8.1k tokens for GPT-4), the plugin could only evaluate one static mobile UI screen at a time.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s4",
                                "text": "This prevents the plugin from evaluating interactivity, design consistency across screens, and task flows.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s5",
                                "text": "Larger UIs, like websites and desktop apps, are more complex and would also exceed this context window size.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s6",
                                "text": "While there are models with larger context windows (Claude 2 and GPT-3.5-16k), they did not produce sufficiently helpful output in our exploration.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s7",
                                "text": "Other LLMs with even smaller context windows, such as Llama 2 and GPT-3.5-turbo, are not suitable for realistic UIs.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s8",
                                "text": "Regarding the studies, they captured the quality of the LLM's feedback, and participants used the plugin to revise mostly high-fidelity mockups.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p1_s9",
                                "text": "These studies did not capture the impact of this plugin's usage on a more realistic design scenario, where designers start with an idea and design a UI mockup based on it.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            }
                        ]
                    },
                    {
                        "full_text": "These limitations suggest exciting opportunities for future work. To start with, we describe potential ways to address the LLM limitations discussed in Section 5.4. We have stored the data on LLM suggestions, ratings, rating explanations, and UI JSONs from both studies (available in the Supplementary Materials). Future work can use this data to fine-tune an LLM to improve performance. Furthermore, to address repetitive suggestions, P5 suggested grouping them into one long suggestion, which could be accomplished by some engineering effort to identify elements of the same type in the UI JSON. Future work should also evaluate the performance of emerging multimodal models (e.g., GPT-4V) with a prompt consisting of both the UI screenshot and JSON representation to see if it improves the LLM's visual understanding of the UI. Models with larger context windows (e.g., GPT-4-turbo) should be evaluated to see if they can enable evaluation of task flows across multiple screens. Such models could also enable use of few-shot or chain-of-thought examples. Once these limitations of the plugin have been addressed, a study could be conducted where participants use this plugin to assist in creating a design from a given prompt. This study would more realistically simulate the tool's usage in practice.",
                        "sentences": [
                            {
                                "id": "sec-54_p2_s1",
                                "text": "These limitations suggest exciting opportunities for future work.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s2",
                                "text": "To start with, we describe potential ways to address the LLM limitations discussed in Section 5.4.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s3",
                                "text": "We have stored the data on LLM suggestions, ratings, rating explanations, and UI JSONs from both studies (available in the Supplementary Materials).",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s4",
                                "text": "Future work can use this data to fine-tune an LLM to improve performance.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s5",
                                "text": "Furthermore, to address repetitive suggestions, P5 suggested grouping them into one long suggestion, which could be accomplished by some engineering effort to identify elements of the same type in the UI JSON.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s6",
                                "text": "Future work should also evaluate the performance of emerging multimodal models (e.g., GPT-4V) with a prompt consisting of both the UI screenshot and JSON representation to see if it improves the LLM's visual understanding of the UI.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s7",
                                "text": "Models with larger context windows (e.g., GPT-4-turbo) should be evaluated to see if they can enable evaluation of task flows across multiple screens.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s8",
                                "text": "Such models could also enable use of few-shot or chain-of-thought examples.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s9",
                                "text": "Once these limitations of the plugin have been addressed, a study could be conducted where participants use this plugin to assist in creating a design from a given prompt.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-54_p2_s10",
                                "text": "This study would more realistically simulate the tool's usage in practice.",
                                "context": "Section 7: LIMITATIONS AND FUTURE WORK",
                                "associated_visual": "fig11"
                            }
                        ]
                    }
                ],
                "lists": [],
                "subsections": []
            },
            {
                "id": "sec-55",
                "section_number": "8",
                "title": "CONCLUSION",
                "paragraphs": [
                    {
                        "full_text": "We designed and built a Figma plugin that uses LLMs to automate the heuristic evaluation of Figma mockups with arbitrary text-based design guidelines. After determining the optimal LLM for this task (GPT-4), we investigated its capability to automate heuristic evaluation through a study where three design experts rated the accuracy and helpfulness of GPT-4 generated design suggestions for 51 UIs, and also compared its feedback with those provided by human experts. Finally, we explored how this tool can fit into existing design practice via a study where 12 design experts used this tool to iteratively refine UIs, assessed the generated feedback, and discussed their experiences working with the plugin. We found that GPT-4 generally performs well on poor UI designs, but its utility decreases through iterations of revising the design. Participants generally do not find the LLM's mistakes to be an issue, as they are easily detected by human designers. Most participants would already use this tool as part of their design practice.",
                        "sentences": [
                            {
                                "id": "sec-55_p1_s1",
                                "text": "We designed and built a Figma plugin that uses LLMs to automate the heuristic evaluation of Figma mockups with arbitrary text-based design guidelines.",
                                "context": "Section 8: CONCLUSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-55_p1_s2",
                                "text": "After determining the optimal LLM for this task (GPT-4), we investigated its capability to automate heuristic evaluation through a study where three design experts rated the accuracy and helpfulness of GPT-4 generated design suggestions for 51 UIs, and also compared its feedback with those provided by human experts.",
                                "context": "Section 8: CONCLUSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-55_p1_s3",
                                "text": "Finally, we explored how this tool can fit into existing design practice via a study where 12 design experts used this tool to iteratively refine UIs, assessed the generated feedback, and discussed their experiences working with the plugin.",
                                "context": "Section 8: CONCLUSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-55_p1_s4",
                                "text": "We found that GPT-4 generally performs well on poor UI designs, but its utility decreases through iterations of revising the design.",
                                "context": "Section 8: CONCLUSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-55_p1_s5",
                                "text": "Participants generally do not find the LLM's mistakes to be an issue, as they are easily detected by human designers.",
                                "context": "Section 8: CONCLUSION",
                                "associated_visual": "fig11"
                            },
                            {
                                "id": "sec-55_p1_s6",
                                "text": "Most participants would already use this tool as part of their design practice.",
                                "context": "Section 8: CONCLUSION",
                                "associated_visual": "fig11"
                            }
                        ]
                    }
                ],
                "lists": [],
                "subsections": []
            }
        ],
        "visual_elements": [
            {
                "type": "table",
                "id": "table_2aa6ab7a",
                "sourceline": 162,
                "headers": [],
                "rows": [
                    [
                        ""
                    ],
                    [
                        "\u2630 Article Navigation"
                    ]
                ],
                "caption": {
                    "full_text": "",
                    "sentences": []
                }
            },
            {
                "type": "figure",
                "id": "fig1",
                "sourceline": 204,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig1.jpg",
                    "alt": "Figure 1"
                },
                "caption": {
                    "full_text": "Figure 1: Diagram illustrating the UI prototyping workflow using this plugin. First, the designer prototypes the UI in Figma (Box A) and then runs the plugin (Arrow A1). The designer then selects the guidelines to use for evaluation (Box B) and runs the evaluation with the selected guidelines (Arrow A2). The plugin obtains evaluation results from the LLM and renders them in an interpretable format (Box C). The designer uses these results to update their design and reruns the evaluation (Arrow A3). The designer iteratively revises their Figma UI mockup, following this process, until they have achieved the desired result.",
                    "sentences": [
                        {
                            "id": "fig1_caption_s1",
                            "text": "Figure 1: Diagram illustrating the UI prototyping workflow using this plugin.",
                            "context": "Caption of Figure fig1",
                            "associated_visual": "fig1"
                        },
                        {
                            "id": "fig1_caption_s2",
                            "text": "First, the designer prototypes the UI in Figma (Box A) and then runs the plugin (Arrow A1).",
                            "context": "Caption of Figure fig1",
                            "associated_visual": "fig1"
                        },
                        {
                            "id": "fig1_caption_s3",
                            "text": "The designer then selects the guidelines to use for evaluation (Box B) and runs the evaluation with the selected guidelines (Arrow A2).",
                            "context": "Caption of Figure fig1",
                            "associated_visual": "fig1"
                        },
                        {
                            "id": "fig1_caption_s4",
                            "text": "The plugin obtains evaluation results from the LLM and renders them in an interpretable format (Box C).",
                            "context": "Caption of Figure fig1",
                            "associated_visual": "fig1"
                        },
                        {
                            "id": "fig1_caption_s5",
                            "text": "The designer uses these results to update their design and reruns the evaluation (Arrow A3).",
                            "context": "Caption of Figure fig1",
                            "associated_visual": "fig1"
                        },
                        {
                            "id": "fig1_caption_s6",
                            "text": "The designer iteratively revises their Figma UI mockup, following this process, until they have achieved the desired result.",
                            "context": "Caption of Figure fig1",
                            "associated_visual": "fig1"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig2",
                "sourceline": 306,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig2.jpg",
                    "alt": "Figure 2"
                },
                "caption": {
                    "full_text": "Figure 2: Illustration of plugin interactions that contextualize text feedback with the UI. \u201cA\u201d shows that clicking on a link in the violation text selects the corresponding group or element in the Figma mockup and Layers panel. \u201cB\u201d shows the \u201cclick to focus\u201d feature, where clicking on a violation fades the other violations and draws a box around the corresponding group in the UI screenshot. \u201cC\u201d illustrates that hovering over a group or element link draws a blue box around the corresponding element in the screenshot. \u201cD\u201d points out that clicking on the \u2018X\u2019 icon of a violation hides it and adds this feedback to the LLM prompt for the next round of evaluation.",
                    "sentences": [
                        {
                            "id": "fig2_caption_s1",
                            "text": "Figure 2: Illustration of plugin interactions that contextualize text feedback with the UI. \u201cA\u201d shows that clicking on a link in the violation text selects the corresponding group or element in the Figma mockup and Layers panel. \u201cB\u201d shows the \u201cclick to focus\u201d feature, where clicking on a violation fades the other violations and draws a box around the corresponding group in the UI screenshot. \u201cC\u201d illustrates that hovering over a group or element link draws a blue box around the corresponding element in the screenshot. \u201cD\u201d points out that clicking on the \u2018X\u2019 icon of a violation hides it and adds this feedback to the LLM prompt for the next round of evaluation.",
                            "context": "Caption of Figure fig2",
                            "associated_visual": "fig2"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig3",
                "sourceline": 323,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig3.jpg",
                    "alt": "Figure 3"
                },
                "caption": {
                    "full_text": "Figure 3: Our LLM-based plugin system architecture. The designer prototypes a UI in Figma (Box 1), and the plugin generates a UI representation to send to an LLM (3). The designer also selects heuristics/guidelines to use for evaluating the prototype (2), and a prompt containing the UI representation (in JSON) and guidelines is created and sent to the LLM (4). After identifying all the guideline violations, another LLM query is made to rephrase the guideline violations into constructive design advice (4). The LLM response is then programmatically parsed (5), and the plugin produces an interpretable representation of the response to display (6). The designer dismisses incorrect suggestions, which are incorporated in the LLM prompt for the next round of evaluation, if there is room in the context window (7).",
                    "sentences": [
                        {
                            "id": "fig3_caption_s1",
                            "text": "Figure 3: Our LLM-based plugin system architecture.",
                            "context": "Caption of Figure fig3",
                            "associated_visual": "fig3"
                        },
                        {
                            "id": "fig3_caption_s2",
                            "text": "The designer prototypes a UI in Figma (Box 1), and the plugin generates a UI representation to send to an LLM (3).",
                            "context": "Caption of Figure fig3",
                            "associated_visual": "fig3"
                        },
                        {
                            "id": "fig3_caption_s3",
                            "text": "The designer also selects heuristics/guidelines to use for evaluating the prototype (2), and a prompt containing the UI representation (in JSON) and guidelines is created and sent to the LLM (4).",
                            "context": "Caption of Figure fig3",
                            "associated_visual": "fig3"
                        },
                        {
                            "id": "fig3_caption_s4",
                            "text": "After identifying all the guideline violations, another LLM query is made to rephrase the guideline violations into constructive design advice (4).",
                            "context": "Caption of Figure fig3",
                            "associated_visual": "fig3"
                        },
                        {
                            "id": "fig3_caption_s5",
                            "text": "The LLM response is then programmatically parsed (5), and the plugin produces an interpretable representation of the response to display (6).",
                            "context": "Caption of Figure fig3",
                            "associated_visual": "fig3"
                        },
                        {
                            "id": "fig3_caption_s6",
                            "text": "The designer dismisses incorrect suggestions, which are incorporated in the LLM prompt for the next round of evaluation, if there is room in the context window (7).",
                            "context": "Caption of Figure fig3",
                            "associated_visual": "fig3"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig4",
                "sourceline": 329,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig4.jpg",
                    "alt": "Figure 4"
                },
                "caption": {
                    "full_text": "Figure 4: An example portion of a UI JSON. It has a tree structure, where each node has a list of child nodes (the \u201cchildren\u201d field). Each node in this JSON is color-coded with its corresponding group or element in the UI screenshot. The node named \u201clyft event photo and logo\u201d is a group (\u201ctype: GROUP\u201d) consisting of a photo of the live chat event (\u201clyft live chat event photo\u201d) and the Lyft logo (\u201clyft logo\u201d). The JSON node for the photo contains its location information (\u201cbounds\u201d), type (\u201cIMAGE\u201d), and unique identifier (\u201cid\u201d). The JSON node for \u201clyft logo\u201d contains its location and some stylistic information, like the stroke color and stroke weight for its white border.",
                    "sentences": [
                        {
                            "id": "fig4_caption_s1",
                            "text": "Figure 4: An example portion of a UI JSON.",
                            "context": "Caption of Figure fig4",
                            "associated_visual": "fig4"
                        },
                        {
                            "id": "fig4_caption_s2",
                            "text": "It has a tree structure, where each node has a list of child nodes (the \u201cchildren\u201d field).",
                            "context": "Caption of Figure fig4",
                            "associated_visual": "fig4"
                        },
                        {
                            "id": "fig4_caption_s3",
                            "text": "Each node in this JSON is color-coded with its corresponding group or element in the UI screenshot.",
                            "context": "Caption of Figure fig4",
                            "associated_visual": "fig4"
                        },
                        {
                            "id": "fig4_caption_s4",
                            "text": "The node named \u201clyft event photo and logo\u201d is a group (\u201ctype: GROUP\u201d) consisting of a photo of the live chat event (\u201clyft live chat event photo\u201d) and the Lyft logo (\u201clyft logo\u201d).",
                            "context": "Caption of Figure fig4",
                            "associated_visual": "fig4"
                        },
                        {
                            "id": "fig4_caption_s5",
                            "text": "The JSON node for the photo contains its location information (\u201cbounds\u201d), type (\u201cIMAGE\u201d), and unique identifier (\u201cid\u201d).",
                            "context": "Caption of Figure fig4",
                            "associated_visual": "fig4"
                        },
                        {
                            "id": "fig4_caption_s6",
                            "text": "The JSON node for \u201clyft logo\u201d contains its location and some stylistic information, like the stroke color and stroke weight for its white border.",
                            "context": "Caption of Figure fig4",
                            "associated_visual": "fig4"
                        }
                    ]
                }
            },
            {
                "type": "table",
                "id": "table1",
                "sourceline": 357,
                "headers": [],
                "rows": [
                    [
                        "Prompt Condition",
                        "Total Violations",
                        "Helpful Violations"
                    ],
                    [
                        "Complete (Plugin)",
                        "63",
                        "38"
                    ],
                    [
                        "One Call",
                        "62",
                        "31"
                    ],
                    [
                        "No Heuristics",
                        "50",
                        "14"
                    ],
                    [
                        "General UI Feedback",
                        "57",
                        "24"
                    ],
                    [
                        "LLM",
                        "Total Violations",
                        "Helpful Violations"
                    ],
                    [
                        "GPT-4 (Plugin)",
                        "63",
                        "38"
                    ],
                    [
                        "GPT-3.5-16k",
                        "228",
                        "23"
                    ],
                    [
                        "Claude 2",
                        "7",
                        "1"
                    ],
                    [
                        "PaLM 2",
                        "12",
                        "3"
                    ]
                ],
                "caption": {
                    "full_text": "Table 1: The top table compares the total number of violations and the number of helpful violations (based on the authors\u2019 judgement) found in 12 UI mockups for different prompt compositions. The \u201cComplete (Plugin)\u201d condition refers to the prompt composition used in the plugin. The bottom table compares the total number of violations and the number of helpful violations (based on the authors\u2019 judgement) found in the 12 UI mockups by each LLM, with GPT-4 being used in the plugin.",
                    "sentences": [
                        {
                            "id": "table1_caption_s1",
                            "text": "Table 1: The top table compares the total number of violations and the number of helpful violations (based on the authors\u2019 judgement) found in 12 UI mockups for different prompt compositions.",
                            "context": "Caption of Table table1",
                            "associated_visual": "table1"
                        },
                        {
                            "id": "table1_caption_s2",
                            "text": "The \u201cComplete (Plugin)\u201d condition refers to the prompt composition used in the plugin.",
                            "context": "Caption of Table table1",
                            "associated_visual": "table1"
                        },
                        {
                            "id": "table1_caption_s3",
                            "text": "The bottom table compares the total number of violations and the number of helpful violations (based on the authors\u2019 judgement) found in the 12 UI mockups by each LLM, with GPT-4 being used in the plugin.",
                            "context": "Caption of Table table1",
                            "associated_visual": "table1"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig5",
                "sourceline": 443,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig5.jpg",
                    "alt": "Figure 5"
                },
                "caption": {
                    "full_text": "Figure 5: An illustration of the formats of the three studies. The Performance Study consists of 3 raters evaluating the accuracy and helpfulness of GPT-4-generated suggestions for 51 UI mockups. The Heuristic Evaluation Study with Human Experts consists of 12 design experts, who each looked for guideline violations in 6 UIs, and finishes with an interview asking them to compare their violations with those found by the LLM. Finally, the Iterative Usage study comprises of another group of 12 design experts, each working with 3 UI mockups. For each mockup, the expert iteratively revises the design based on the LLM's valid suggestions and rates the LLM's feedback, going through 2-3 rounds of this per UI. The Usage study concludes with an interview about the expert's experience with the tool.",
                    "sentences": [
                        {
                            "id": "fig5_caption_s1",
                            "text": "Figure 5: An illustration of the formats of the three studies.",
                            "context": "Caption of Figure fig5",
                            "associated_visual": "fig5"
                        },
                        {
                            "id": "fig5_caption_s2",
                            "text": "The Performance Study consists of 3 raters evaluating the accuracy and helpfulness of GPT-4-generated suggestions for 51 UI mockups.",
                            "context": "Caption of Figure fig5",
                            "associated_visual": "fig5"
                        },
                        {
                            "id": "fig5_caption_s3",
                            "text": "The Heuristic Evaluation Study with Human Experts consists of 12 design experts, who each looked for guideline violations in 6 UIs, and finishes with an interview asking them to compare their violations with those found by the LLM.",
                            "context": "Caption of Figure fig5",
                            "associated_visual": "fig5"
                        },
                        {
                            "id": "fig5_caption_s4",
                            "text": "Finally, the Iterative Usage study comprises of another group of 12 design experts, each working with 3 UI mockups.",
                            "context": "Caption of Figure fig5",
                            "associated_visual": "fig5"
                        },
                        {
                            "id": "fig5_caption_s5",
                            "text": "For each mockup, the expert iteratively revises the design based on the LLM's valid suggestions and rates the LLM's feedback, going through 2-3 rounds of this per UI.",
                            "context": "Caption of Figure fig5",
                            "associated_visual": "fig5"
                        },
                        {
                            "id": "fig5_caption_s6",
                            "text": "The Usage study concludes with an interview about the expert's experience with the tool.",
                            "context": "Caption of Figure fig5",
                            "associated_visual": "fig5"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig6",
                "sourceline": 489,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig6.jpg",
                    "alt": "Figure 6"
                },
                "caption": {
                    "full_text": "Figure 6: Histogram showing the number of ratings in each category for accuracy and helpfulness, from the 3 participants in the Performance Study. For accuracy, the scale is: \u201c1 - not accurate\u201d, \u201c2 - partially accurate\u201d, and \u201c3 - accurate\u201d. The scale for helpfulness ranges from \u201c1 - not at all helpful\u201d to \u201c5 - very helpful\u201d. The rating data is also visualized as horizontal bar charts for this study and the Usage Study.",
                    "sentences": [
                        {
                            "id": "fig6_caption_s1",
                            "text": "Figure 6: Histogram showing the number of ratings in each category for accuracy and helpfulness, from the 3 participants in the Performance Study.",
                            "context": "Caption of Figure fig6",
                            "associated_visual": "fig6"
                        },
                        {
                            "id": "fig6_caption_s2",
                            "text": "For accuracy, the scale is: \u201c1 - not accurate\u201d, \u201c2 - partially accurate\u201d, and \u201c3 - accurate\u201d.",
                            "context": "Caption of Figure fig6",
                            "associated_visual": "fig6"
                        },
                        {
                            "id": "fig6_caption_s3",
                            "text": "The scale for helpfulness ranges from \u201c1 - not at all helpful\u201d to \u201c5 - very helpful\u201d.",
                            "context": "Caption of Figure fig6",
                            "associated_visual": "fig6"
                        },
                        {
                            "id": "fig6_caption_s4",
                            "text": "The rating data is also visualized as horizontal bar charts for this study and the Usage Study.",
                            "context": "Caption of Figure fig6",
                            "associated_visual": "fig6"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig7",
                "sourceline": 495,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig7.jpg",
                    "alt": "Figure 7"
                },
                "caption": {
                    "full_text": "Figure 7: Horizontal bar charts showing the distribution of ratings from the Performance Study for each individual guideline. The ratings for accuracy are in the top row, and helpfulness is in the bottom row, and each chart has a horizontal black line depicting the average rating. We highlight several guidelines with high ratings in green, such as \u201cConsistency and Standards\u201d from Nielsen Norman's 10 Usability Heuristics. We used orange to highlight an average performing guideline \u2013 \u201cEmphasis\u201d (from CrowdCrit), which had bimodal ratings for accuracy and helpfulness. Finally, we used red to highlight the worst performing guideline \u2013 \u201cAesthetic and Minimalist Design\u201d, which had generally poor accuracy and helpfulness ratings.",
                    "sentences": [
                        {
                            "id": "fig7_caption_s1",
                            "text": "Figure 7: Horizontal bar charts showing the distribution of ratings from the Performance Study for each individual guideline.",
                            "context": "Caption of Figure fig7",
                            "associated_visual": "fig7"
                        },
                        {
                            "id": "fig7_caption_s2",
                            "text": "The ratings for accuracy are in the top row, and helpfulness is in the bottom row, and each chart has a horizontal black line depicting the average rating.",
                            "context": "Caption of Figure fig7",
                            "associated_visual": "fig7"
                        },
                        {
                            "id": "fig7_caption_s3",
                            "text": "We highlight several guidelines with high ratings in green, such as \u201cConsistency and Standards\u201d from Nielsen Norman's 10 Usability Heuristics.",
                            "context": "Caption of Figure fig7",
                            "associated_visual": "fig7"
                        },
                        {
                            "id": "fig7_caption_s4",
                            "text": "We used orange to highlight an average performing guideline \u2013 \u201cEmphasis\u201d (from CrowdCrit), which had bimodal ratings for accuracy and helpfulness.",
                            "context": "Caption of Figure fig7",
                            "associated_visual": "fig7"
                        },
                        {
                            "id": "fig7_caption_s5",
                            "text": "Finally, we used red to highlight the worst performing guideline \u2013 \u201cAesthetic and Minimalist Design\u201d, which had generally poor accuracy and helpfulness ratings.",
                            "context": "Caption of Figure fig7",
                            "associated_visual": "fig7"
                        }
                    ]
                }
            },
            {
                "type": "table",
                "id": "table2",
                "sourceline": 507,
                "headers": [
                    "Performance Metrics",
                    "GPT-4",
                    "Human Evaluator (Avg.)"
                ],
                "rows": [
                    [
                        "Performance Metrics",
                        "GPT-4",
                        "Human Evaluator (Avg.)"
                    ],
                    [
                        "Precision",
                        "0.603",
                        "0.829"
                    ],
                    [
                        "Recall",
                        "0.380",
                        "0.336"
                    ],
                    [
                        "F1",
                        "0.466",
                        "0.478"
                    ]
                ],
                "caption": {
                    "full_text": "Table 2: Table showing the Precision, Recall, and F1 scores of GPT-4 and an individual human evaluator, computed from the ground truth dataset. The metrics for the human evaluator is computed by averaging these metrics across all participants in the study (for the 6 UIs they each evaluated).",
                    "sentences": [
                        {
                            "id": "table2_caption_s1",
                            "text": "Table 2: Table showing the Precision, Recall, and F1 scores of GPT-4 and an individual human evaluator, computed from the ground truth dataset.",
                            "context": "Caption of Table table2",
                            "associated_visual": "table2"
                        },
                        {
                            "id": "table2_caption_s2",
                            "text": "The metrics for the human evaluator is computed by averaging these metrics across all participants in the study (for the 6 UIs they each evaluated).",
                            "context": "Caption of Table table2",
                            "associated_visual": "table2"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig8",
                "sourceline": 550,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig8.jpg",
                    "alt": "Figure 8"
                },
                "caption": {
                    "full_text": "Figure 8: Horizontal bar charts showing the distribution of ratings for each round of evaluation in the Usage study. The ratings are for suggestions from all participants and sets of guidelines. The average rating and standard deviation is marked for each round, and there is a general downward trend in performance as the number of rounds increases.",
                    "sentences": [
                        {
                            "id": "fig8_caption_s1",
                            "text": "Figure 8: Horizontal bar charts showing the distribution of ratings for each round of evaluation in the Usage study.",
                            "context": "Caption of Figure fig8",
                            "associated_visual": "fig8"
                        },
                        {
                            "id": "fig8_caption_s2",
                            "text": "The ratings are for suggestions from all participants and sets of guidelines.",
                            "context": "Caption of Figure fig8",
                            "associated_visual": "fig8"
                        },
                        {
                            "id": "fig8_caption_s3",
                            "text": "The average rating and standard deviation is marked for each round, and there is a general downward trend in performance as the number of rounds increases.",
                            "context": "Caption of Figure fig8",
                            "associated_visual": "fig8"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig9",
                "sourceline": 566,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig9.jpg",
                    "alt": "Figure 9"
                },
                "caption": {
                    "full_text": "Figure 9: Examples of GPT-4 suggestions that all participants found very helpful or unhelpful, along with their corresponding UIs above (with the relevant group marked). The suggestions for UIs A and B received ratings of 5 for helpfulness and were rated as accurate by all three participants (from the Usage study). The \u201cContact:\u201d field for UI B is slightly misaligned from the other fields, which GPT-4 caught. UIs C and D were rated 1 for helpfulness by all three participants. For UI C, the LLM stated that the line thickness was uneven under the \u201cADS\u201d and \u201cFAVORITES\u201d tab, which is technically accurate (and some participants rated it as accurate) but unhelpful as the uneven line thickness is meant to indicate the selected tab.",
                    "sentences": [
                        {
                            "id": "fig9_caption_s1",
                            "text": "Figure 9: Examples of GPT-4 suggestions that all participants found very helpful or unhelpful, along with their corresponding UIs above (with the relevant group marked).",
                            "context": "Caption of Figure fig9",
                            "associated_visual": "fig9"
                        },
                        {
                            "id": "fig9_caption_s2",
                            "text": "The suggestions for UIs A and B received ratings of 5 for helpfulness and were rated as accurate by all three participants (from the Usage study).",
                            "context": "Caption of Figure fig9",
                            "associated_visual": "fig9"
                        },
                        {
                            "id": "fig9_caption_s3",
                            "text": "The \u201cContact:\u201d field for UI B is slightly misaligned from the other fields, which GPT-4 caught.",
                            "context": "Caption of Figure fig9",
                            "associated_visual": "fig9"
                        },
                        {
                            "id": "fig9_caption_s4",
                            "text": "UIs C and D were rated 1 for helpfulness by all three participants.",
                            "context": "Caption of Figure fig9",
                            "associated_visual": "fig9"
                        },
                        {
                            "id": "fig9_caption_s5",
                            "text": "For UI C, the LLM stated that the line thickness was uneven under the \u201cADS\u201d and \u201cFAVORITES\u201d tab, which is technically accurate (and some participants rated it as accurate) but unhelpful as the uneven line thickness is meant to indicate the selected tab.",
                            "context": "Caption of Figure fig9",
                            "associated_visual": "fig9"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig10",
                "sourceline": 607,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig10.jpg",
                    "alt": "Figure 10"
                },
                "caption": {
                    "full_text": "Figure 10: The screenshot on the left compares a high-level \u201cglobal\u201d violation found by a human expert with a similar, but more specific, violation found by the LLM. The right screenshot contains two violations found by human experts that require advanced visual understanding of the UI.",
                    "sentences": [
                        {
                            "id": "fig10_caption_s1",
                            "text": "Figure 10: The screenshot on the left compares a high-level \u201cglobal\u201d violation found by a human expert with a similar, but more specific, violation found by the LLM.",
                            "context": "Caption of Figure fig10",
                            "associated_visual": "fig10"
                        },
                        {
                            "id": "fig10_caption_s2",
                            "text": "The right screenshot contains two violations found by human experts that require advanced visual understanding of the UI.",
                            "context": "Caption of Figure fig10",
                            "associated_visual": "fig10"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig11",
                "sourceline": 655,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig11.jpg",
                    "alt": "Figure 11"
                },
                "caption": {
                    "full_text": "Figure 11: Plots of the average accuracy and helpfulness rating per round from P2, who used the CrowdCrit guidelines to evaluate the UI shown above. Paraphrased GPT-4 suggestions from each round and their average accuracy and helpfulness ratings are also provided. The suggestions P2 used to edit the UI are highlighted in gray, and the corresponding changes are annotated in the revised UI. Participants made at most 2 rounds of edits, so the \u201cFeedback for UI After Second Revision\u201d was never used for edits. Note that participants were instructed to approach this task as if they were using this plugin for their own design work and to put as much effort into the edits as they would like. This sometimes led them to fix issues in the design beyond what the LLM explicitly stated. The UI's visual design improves per round, while the average accuracy and helpful ratings decrease.",
                    "sentences": [
                        {
                            "id": "fig11_caption_s1",
                            "text": "Figure 11: Plots of the average accuracy and helpfulness rating per round from P2, who used the CrowdCrit guidelines to evaluate the UI shown above.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        },
                        {
                            "id": "fig11_caption_s2",
                            "text": "Paraphrased GPT-4 suggestions from each round and their average accuracy and helpfulness ratings are also provided.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        },
                        {
                            "id": "fig11_caption_s3",
                            "text": "The suggestions P2 used to edit the UI are highlighted in gray, and the corresponding changes are annotated in the revised UI.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        },
                        {
                            "id": "fig11_caption_s4",
                            "text": "Participants made at most 2 rounds of edits, so the \u201cFeedback for UI After Second Revision\u201d was never used for edits.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        },
                        {
                            "id": "fig11_caption_s5",
                            "text": "Note that participants were instructed to approach this task as if they were using this plugin for their own design work and to put as much effort into the edits as they would like.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        },
                        {
                            "id": "fig11_caption_s6",
                            "text": "This sometimes led them to fix issues in the design beyond what the LLM explicitly stated.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        },
                        {
                            "id": "fig11_caption_s7",
                            "text": "The UI's visual design improves per round, while the average accuracy and helpful ratings decrease.",
                            "context": "Caption of Figure fig11",
                            "associated_visual": "fig11"
                        }
                    ]
                }
            },
            {
                "type": "figure",
                "id": "fig12",
                "sourceline": 800,
                "image": {
                    "src": "cms/attachment/html/10.1145/3613904.3642782/assets/html/images/chi24-886-fig12.jpg",
                    "alt": "Figure 12"
                },
                "caption": {
                    "full_text": "Figure 12: Diagram illustrating the components of each prompt. The Heuristic Evaluation Prompt and the LLM Eval. Response Rephrasing Prompt form a chain, where the Rephrasing Prompt takes the LLM response from the Heuristic Evaluation prompt and instructs the LLM to rephrase it. The Heuristic Evaluation prompt starts with system instructions to guide the LLM's behavior and contains the set of guidelines for evaluation. It is followed by a conversation history with incorrect/unhelpful violations found by the LLM (as \u201cAssistant\u201d) that were denoted by the designer, a \u201cUser\u201d query telling the LLM that these violations were wrong and to self-reflect, and the LLM's response to the self-reflection. There may be zero to multiple sets of this conversation, depending on the number of evaluation rounds. The final component is the user message, which contains the UI JSON, instructions to identify guideline violations, a short description of the content available in the UI JSON, and specific instructions to avoid common errors. The Rephrasing prompt contains system instructions that direct the LLM to constructively rephrase the violation explanation (following [48]) and also guides the LLM to format the response correctly. The user query contains the LLM Eval. response with the identified violations.",
                    "sentences": [
                        {
                            "id": "fig12_caption_s1",
                            "text": "Figure 12: Diagram illustrating the components of each prompt.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s2",
                            "text": "The Heuristic Evaluation Prompt and the LLM Eval.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s3",
                            "text": "Response Rephrasing Prompt form a chain, where the Rephrasing Prompt takes the LLM response from the Heuristic Evaluation prompt and instructs the LLM to rephrase it.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s4",
                            "text": "The Heuristic Evaluation prompt starts with system instructions to guide the LLM's behavior and contains the set of guidelines for evaluation.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s5",
                            "text": "It is followed by a conversation history with incorrect/unhelpful violations found by the LLM (as \u201cAssistant\u201d) that were denoted by the designer, a \u201cUser\u201d query telling the LLM that these violations were wrong and to self-reflect, and the LLM's response to the self-reflection.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s6",
                            "text": "There may be zero to multiple sets of this conversation, depending on the number of evaluation rounds.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s7",
                            "text": "The final component is the user message, which contains the UI JSON, instructions to identify guideline violations, a short description of the content available in the UI JSON, and specific instructions to avoid common errors.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s8",
                            "text": "The Rephrasing prompt contains system instructions that direct the LLM to constructively rephrase the violation explanation (following [48]) and also guides the LLM to format the response correctly.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        },
                        {
                            "id": "fig12_caption_s9",
                            "text": "The user query contains the LLM Eval. response with the identified violations.",
                            "context": "Caption of Figure fig12",
                            "associated_visual": "fig12"
                        }
                    ]
                }
            }
        ]
    }
}